{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec25714",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216a0c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88cd66f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Setup\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append('..')\n",
    "\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "from PIL import Image\n",
    "from tqdm import tqdm  # Add this import\n",
    "from datetime import datetime  # Add this too\n",
    "import torch\n",
    "import yaml\n",
    "import os\n",
    "import random\n",
    "from src.utils.visualizer import YOLOVisualizer\n",
    "\n",
    "# Set high DPI for academic quality figures\n",
    "plt.rcParams['figure.dpi'] = 150\n",
    "plt.rcParams['savefig.dpi'] = 300\n",
    "plt.rcParams['font.size'] = 10\n",
    "plt.rcParams['font.family'] = 'sans-serif'\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "from src.evaluation.evaluator import ComprehensiveEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f401db8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: Force Reload Evaluator Module\n",
    "import importlib\n",
    "import sys\n",
    "\n",
    "# Remove the cached module\n",
    "if 'src.evaluation.evaluator' in sys.modules:\n",
    "    del sys.modules['src.evaluation.evaluator']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5763c1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set reproducibility\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542d8481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Configuration\n",
    "from configs.baseline_config import ExperimentConfig\n",
    "\n",
    "# Configure your experiment here\n",
    "config = ExperimentConfig(\n",
    "    dataset='d1',        # Change to 'd2' or 'd3' as needed\n",
    "    task='binary',       # Change to 'species' or 'staging' as needed\n",
    "    model_name='yolov8s',\n",
    "    epochs=10           # Increase to 200 for full training\n",
    ")\n",
    "\n",
    "experiment_name = config.get_experiment_name()\n",
    "print(f\"Experiment: {experiment_name}\")\n",
    "print(f\"Dataset: {config.dataset.upper()}\")\n",
    "print(f\"Task: {config.task}\")\n",
    "print(f\"Model: {config.model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72b6dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagnostic Cell \n",
    "\n",
    "# Check actual structure\n",
    "base = Path(\"/Users/thabangisaka/Downloads/thabang_phd/Experiments/Year 3 Experiments/malaria_experiments\")\n",
    "d1_path = base / \"dataset_d1\"\n",
    "\n",
    "print(\"Checking D1 structure:\")\n",
    "print(f\"D1 exists: {d1_path.exists()}\")\n",
    "\n",
    "if d1_path.exists():\n",
    "    # List top level\n",
    "    print(\"\\nTop level contents:\")\n",
    "    for item in sorted(d1_path.iterdir()):\n",
    "        if item.is_dir():\n",
    "            print(f\"  📁 {item.name}/\")\n",
    "            # List subdirectories\n",
    "            for subitem in sorted(item.iterdir())[:5]:  # Show first 5\n",
    "                if subitem.is_dir():\n",
    "                    print(f\"      📁 {subitem.name}/\")\n",
    "                else:\n",
    "                    print(f\"      📄 {subitem.name}\")\n",
    "            if len(list(item.iterdir())) > 5:\n",
    "                print(f\"      ... and {len(list(item.iterdir())) - 5} more items\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7383358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Prepare Dataset (Handles conversion automatically)\n",
    "from src.utils.paths import get_dataset_paths, verify_dataset\n",
    "\n",
    "# This will automatically convert COCO to YOLO if needed\n",
    "dataset_valid = verify_dataset(config.dataset, config.task)\n",
    "if not dataset_valid:\n",
    "    raise RuntimeError(f\"Dataset preparation failed for {config.dataset}/{config.task}\")\n",
    "\n",
    "# Get the YOLO format path\n",
    "yolo_path = get_dataset_paths(config.dataset, config.task)\n",
    "\n",
    "# Cell 4: Create YAML for YOLO\n",
    "yaml_dir = Path('../configs/data_yamls')\n",
    "yaml_dir.mkdir(exist_ok=True)\n",
    "yaml_path = yaml_dir / f'{config.dataset}_{config.task}.yaml'\n",
    "\n",
    "# Simple YAML pointing to YOLO structure\n",
    "data_yaml = {\n",
    "    'path': str(yolo_path),  # Points to yolo_format/task/\n",
    "    'train': 'train/images',\n",
    "    'val': 'val/images',\n",
    "    'test': 'test/images',\n",
    "    'names': config.get_class_names(),\n",
    "    'nc': len(config.get_class_names())\n",
    "}\n",
    "\n",
    "with open(yaml_path, 'w') as f:\n",
    "    yaml.dump(data_yaml, f, default_flow_style=False)\n",
    "\n",
    "print(f\"Created YAML: {yaml_path}\")\n",
    "print(\"\\nYAML content:\")\n",
    "print(yaml.dump(data_yaml, default_flow_style=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca16f8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Dataset Visualization and Statistics (Modular)\n",
    "\n",
    "# Create visualizer - automatically adapts to task\n",
    "visualizer = YOLOVisualizer(config.get_class_names())\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(f\"DATASET VISUALIZATION: {config.dataset.upper()} - {config.task.upper()}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Visualize samples\n",
    "fig = visualizer.visualize_dataset_samples(yolo_path, num_samples=4)\n",
    "plt.show()\n",
    "\n",
    "# Get and display statistics\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DATASET STATISTICS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "stats = visualizer.get_dataset_stats(yolo_path)\n",
    "\n",
    "# Overall summary\n",
    "total_images = sum(s['images'] for s in stats.values())\n",
    "total_boxes = sum(s['total_boxes'] for s in stats.values())\n",
    "\n",
    "print(f\"\\nOVERALL:\")\n",
    "print(f\"  Total images: {total_images}\")\n",
    "print(f\"  Total annotations: {total_boxes}\")\n",
    "\n",
    "# Class distribution\n",
    "total_per_class = {}\n",
    "for s in stats.values():\n",
    "    for class_id, count in s['class_distribution'].items():\n",
    "        total_per_class[class_id] = total_per_class.get(class_id, 0) + count\n",
    "\n",
    "print(f\"\\n  Class Distribution:\")\n",
    "for class_id in sorted(total_per_class.keys()):\n",
    "    count = total_per_class[class_id]\n",
    "    class_name = config.get_class_names().get(class_id)\n",
    "    percentage = (count/total_boxes*100) if total_boxes > 0 else 0\n",
    "    print(f\"    {class_name}: {count:,} ({percentage:.1f}%)\")\n",
    "\n",
    "# Identify imbalance\n",
    "if len(total_per_class) > 0:\n",
    "    max_count = max(total_per_class.values())\n",
    "    min_count = min(total_per_class.values())\n",
    "    if min_count > 0:\n",
    "        imbalance_ratio = max_count / min_count\n",
    "        print(f\"\\n  Class Imbalance Ratio: {imbalance_ratio:.1f}:1\")\n",
    "\n",
    "# Per-split details\n",
    "for split, split_stats in stats.items():\n",
    "    print(f\"\\n{split.upper()} Split:\")\n",
    "    print(f\"  Images: {split_stats['images']}\")\n",
    "    print(f\"  Total boxes: {split_stats['total_boxes']:,}\")\n",
    "    if visualizer.minority_classes:\n",
    "        print(f\"  Images with minority classes: {split_stats['images_with_minority']}\")\n",
    "    print(f\"  Avg boxes/image: {split_stats['avg_boxes_per_image']:.1f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"READY FOR TRAINING\" if total_boxes > 0 else \"NO ANNOTATIONS FOUND\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59331c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Initialize Model\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Download/load pretrained weights\n",
    "if config.model_name == 'yolov8s':\n",
    "    model = YOLO('yolov8s.pt')\n",
    "elif config.model_name == 'yolov11s':\n",
    "    model = YOLO('yolo11s.pt')  \n",
    "else:\n",
    "    raise ValueError(f\"Model {config.model_name} not implemented yet\")\n",
    "\n",
    "print(f\"Model loaded: {config.model_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3998af44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: W&B Initialization with Real-Time Logging Setup\n",
    "import wandb\n",
    "import time\n",
    "from datetime import datetime\n",
    "import torch\n",
    "\n",
    "# Initialize tracking variables\n",
    "training_metrics = {\n",
    "    'start_time': None,\n",
    "    'end_time': None,\n",
    "    'total_training_time': 0,\n",
    "    'inference_times': [],\n",
    "    'epoch_times': [],\n",
    "    'best_metrics': {}  # Added for tracking best performance during training\n",
    "}\n",
    "\n",
    "if config.use_wandb:\n",
    "    wandb.login(key=config.wandb_key)\n",
    "    \n",
    "    # More comprehensive config with safe attribute access\n",
    "    wandb_config = {\n",
    "        # Model architecture\n",
    "        'model': config.model_name,\n",
    "        'parameters': None,  # Will update after model loads\n",
    "        \n",
    "        # Training hyperparameters\n",
    "        'epochs': config.epochs,\n",
    "        'batch_size': config.batch_size,\n",
    "        'learning_rate': config.lr0,\n",
    "        'optimizer': config.optimizer,\n",
    "        'momentum': config.momentum,\n",
    "        'weight_decay': config.weight_decay,\n",
    "        \n",
    "        # Dataset info\n",
    "        'dataset': config.dataset,\n",
    "        'task': config.task,\n",
    "        'num_classes': len(config.get_class_names()),\n",
    "        'train_images': len(list((yolo_path / 'train' / 'images').glob('*.jpg'))),\n",
    "        'val_images': len(list((yolo_path / 'val' / 'images').glob('*.jpg'))),\n",
    "        'test_images': len(list((yolo_path / 'test' / 'images').glob('*.jpg'))),\n",
    "        \n",
    "        # Hardware info\n",
    "        'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "        'num_workers': getattr(config, 'workers', 8),  # Default to 8 if not set\n",
    "        \n",
    "        # Experiment metadata\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'experiment_type': getattr(config, 'training_strategy', 'no_weights'),\n",
    "        \n",
    "        # Additional configs if they exist\n",
    "        'patience': getattr(config, 'patience', 20),\n",
    "        'warmup_epochs': getattr(config, 'warmup_epochs', 3.0),\n",
    "        'save_period': getattr(config, 'save_period', 10),\n",
    "    }\n",
    "    \n",
    "    # Add strategy to tags if it exists\n",
    "    tags = [config.dataset, config.task, config.model_name]\n",
    "    strategy = getattr(config, 'training_strategy', 'no_weights')\n",
    "    tags.append(strategy)\n",
    "    \n",
    "    # Initialize W&B run\n",
    "    run = wandb.init(\n",
    "        project=config.wandb_project,\n",
    "        name=experiment_name,\n",
    "        config=wandb_config,\n",
    "        tags=tags\n",
    "    )\n",
    "    \n",
    "    # Define custom metrics for proper x-axis tracking in charts\n",
    "    wandb.define_metric(\"epoch\")\n",
    "    wandb.define_metric(\"train/*\", step_metric=\"epoch\")\n",
    "    wandb.define_metric(\"val/*\", step_metric=\"epoch\")\n",
    "    wandb.define_metric(\"metrics/*\", step_metric=\"epoch\")\n",
    "    wandb.define_metric(\"lr/*\", step_metric=\"epoch\")\n",
    "    \n",
    "    print(f\"✓ W&B initialized: {run.name}\")\n",
    "    print(f\"✓ View run at: {run.url}\")\n",
    "    print(f\"✓ Real-time logging configured\")\n",
    "else:\n",
    "    print(\"W&B logging disabled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e4a392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Complete Training with Post-Training W&B Logging\n",
    "from ultralytics import YOLO\n",
    "import time\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb\n",
    "from src.training.strategy_wrapper import create_training_strategy\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(f\"TRAINING PIPELINE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Verify required variables\n",
    "assert 'config' in globals(), \"config not found - run Cell 1 first\"\n",
    "assert 'yolo_path' in globals(), \"yolo_path not found - run Cell 3 first\"\n",
    "\n",
    "# Define yaml_file path\n",
    "yaml_file = Path(f'../configs/data_yamls/{config.dataset}_{config.task}.yaml')\n",
    "if not yaml_file.exists():\n",
    "    raise FileNotFoundError(f\"YAML file not found: {yaml_file}\")\n",
    "print(f\"Using YAML config: {yaml_file}\")\n",
    "\n",
    "# Get class distribution from training data\n",
    "def get_class_distribution(yolo_path):\n",
    "    \"\"\"Calculate class distribution from training labels\"\"\"\n",
    "    distribution = defaultdict(int)\n",
    "    label_dir = yolo_path / 'train' / 'labels'\n",
    "    \n",
    "    for class_id, class_name in config.get_class_names().items():\n",
    "        distribution[class_name] = 0\n",
    "    \n",
    "    for label_file in label_dir.glob('*.txt'):\n",
    "        with open(label_file, 'r') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) >= 5:\n",
    "                    class_id = int(parts[0])\n",
    "                    class_name = config.get_class_names()[class_id]\n",
    "                    distribution[class_name] += 1\n",
    "    \n",
    "    return dict(distribution)\n",
    "\n",
    "# Calculate distribution\n",
    "print(\"\\nCalculating class distribution...\")\n",
    "class_distribution = get_class_distribution(yolo_path)\n",
    "total_annotations = sum(class_distribution.values())\n",
    "\n",
    "print(f\"\\nTraining Set Statistics:\")\n",
    "print(f\"  Total annotations: {total_annotations}\")\n",
    "for class_name, count in class_distribution.items():\n",
    "    percentage = (count / total_annotations * 100) if total_annotations > 0 else 0\n",
    "    print(f\"  {class_name}: {count} ({percentage:.1f}%)\")\n",
    "\n",
    "# Select training strategy\n",
    "TRAINING_STRATEGY = getattr(config, 'training_strategy', 'no_weights')\n",
    "print(f\"\\nSelected Strategy: {TRAINING_STRATEGY}\")\n",
    "\n",
    "# Create strategy\n",
    "strategy = create_training_strategy(TRAINING_STRATEGY, config, class_distribution)\n",
    "strategy_params = strategy.get_training_params()\n",
    "hyperparameter_adjustments = strategy.get_hyperparameter_adjustments()\n",
    "\n",
    "# Update experiment name\n",
    "experiment_name = f\"{config.model_name}_{config.dataset}_{config.task}_{strategy.get_strategy_name()}\"\n",
    "print(f\"\\nExperiment Name: {experiment_name}\")\n",
    "\n",
    "# Update W&B config if active\n",
    "if config.use_wandb:\n",
    "    if not hasattr(wandb, 'run') or wandb.run is None:\n",
    "        raise RuntimeError(\"W&B not initialized. Run Cell 7 first\")\n",
    "    \n",
    "    wandb.config.update({\n",
    "        'training_strategy': strategy.get_strategy_name(),\n",
    "        'class_distribution': class_distribution,\n",
    "        'minority_classes': [config.get_class_names()[i] for i in strategy.minority_classes],\n",
    "        'class_weights': strategy.class_weights.tolist(),\n",
    "        'total_training_annotations': total_annotations\n",
    "    })\n",
    "    \n",
    "    wandb.run.name = experiment_name\n",
    "\n",
    "# Track training start time\n",
    "training_start_time = time.time()\n",
    "\n",
    "# Load model\n",
    "print(f\"\\nLoading model: {config.model_name}.pt\")\n",
    "model = YOLO(f'{config.model_name}.pt')\n",
    "\n",
    "# Get model info for W&B\n",
    "if config.use_wandb:\n",
    "    try:\n",
    "        total_params = sum(p.numel() for p in model.model.parameters())\n",
    "        trainable_params = sum(p.numel() for p in model.model.parameters() if p.requires_grad)\n",
    "        wandb.config.update({\n",
    "            'total_parameters': total_params,\n",
    "            'trainable_parameters': trainable_params,\n",
    "            'model_size_mb': total_params * 4 / (1024 * 1024)\n",
    "        })\n",
    "        print(f\"Model Parameters: {total_params:,} total, {trainable_params:,} trainable\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# Apply hyperparameter adjustments\n",
    "for param, value in hyperparameter_adjustments.items():\n",
    "    if hasattr(config, param):\n",
    "        old_value = getattr(config, param)\n",
    "        print(f\"Adjusting {param}: {old_value} -> {value}\")\n",
    "        setattr(config, param, value)\n",
    "\n",
    "# Prepare training arguments (NO W&B project parameter)\n",
    "# Cell 8: Complete Training with Post-Training W&B Logging (UPDATED)\n",
    "from ultralytics import YOLO\n",
    "import time\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb\n",
    "from src.training.strategy_wrapper import create_training_strategy\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(f\"TRAINING PIPELINE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# [All setup code remains the same until train_args]\n",
    "\n",
    "# Prepare training arguments with FIXED folder structure\n",
    "train_args = {\n",
    "    'data': str(yaml_file),\n",
    "    'epochs': config.epochs,\n",
    "    'imgsz': config.imgsz,\n",
    "    'batch': config.batch_size,\n",
    "    'name': experiment_name,\n",
    "    'project': '../runs/detect',  # ADDED: Controls where YOLO saves to avoid nested folders\n",
    "    'device': 'cpu' if not torch.cuda.is_available() else None,\n",
    "    'patience': getattr(config, 'patience', 20),\n",
    "    'save': True,\n",
    "    'save_period': getattr(config, 'save_period', 10),\n",
    "    'val': getattr(config, 'val', True),\n",
    "    'amp': getattr(config, 'amp', True),\n",
    "    'exist_ok': getattr(config, 'exist_ok', True),\n",
    "    'seed': getattr(config, 'seed', 42),\n",
    "    'deterministic': getattr(config, 'deterministic', True),\n",
    "    'single_cls': getattr(config, 'single_cls', False),\n",
    "    'rect': getattr(config, 'rect', False),\n",
    "    'cos_lr': getattr(config, 'cos_lr', False),\n",
    "    'resume': getattr(config, 'resume', False),\n",
    "    \n",
    "    # [Rest of parameters remain the same]\n",
    "    'optimizer': getattr(config, 'optimizer', 'SGD'),\n",
    "    'lr0': getattr(config, 'lr0', 0.005),\n",
    "    'lrf': getattr(config, 'lrf', 0.01),\n",
    "    'momentum': getattr(config, 'momentum', 0.95),\n",
    "    'weight_decay': getattr(config, 'weight_decay', 0.0005),\n",
    "    'warmup_epochs': getattr(config, 'warmup_epochs', 3.0),\n",
    "    'warmup_momentum': getattr(config, 'warmup_momentum', 0.8),\n",
    "    'warmup_bias_lr': getattr(config, 'warmup_bias_lr', 0.1),\n",
    "    \n",
    "    'box': strategy_params.get('box', 7.5),\n",
    "    'cls': strategy_params.get('cls', 0.5),\n",
    "    'dfl': strategy_params.get('dfl', 1.5),\n",
    "    \n",
    "    'hsv_h': getattr(config, 'hsv_h', 0.015),\n",
    "    'hsv_s': getattr(config, 'hsv_s', 0.7),\n",
    "    'hsv_v': getattr(config, 'hsv_v', 0.4),\n",
    "    'degrees': getattr(config, 'degrees', 0.0),\n",
    "    'translate': getattr(config, 'translate', 0.1),\n",
    "    'scale': getattr(config, 'scale', 0.5),\n",
    "    'shear': getattr(config, 'shear', 0.0),\n",
    "    'perspective': getattr(config, 'perspective', 0.0),\n",
    "    'flipud': getattr(config, 'flipud', 0.0),\n",
    "    'fliplr': getattr(config, 'fliplr', 0.5),\n",
    "    'mosaic': getattr(config, 'mosaic', 1.0),\n",
    "    'mixup': getattr(config, 'mixup', 0.0),\n",
    "    'copy_paste': getattr(config, 'copy_paste', 0.0),\n",
    "    \n",
    "    'overlap_mask': getattr(config, 'overlap_mask', True),\n",
    "    'mask_ratio': getattr(config, 'mask_ratio', 4),\n",
    "    'dropout': getattr(config, 'dropout', 0.0),\n",
    "    \n",
    "    'plots': getattr(config, 'plots', True),\n",
    "    'verbose': getattr(config, 'verbose', True),\n",
    "    \n",
    "    'workers': getattr(config, 'workers', 8),\n",
    "}\n",
    "\n",
    "\n",
    "print(f\"\\nStarting training for {config.epochs} epochs...\")\n",
    "print(f\"Strategy: {strategy.get_strategy_name()}\")\n",
    "print(f\"Loss weights - Box: {train_args['box']}, Cls: {train_args['cls']}, DFL: {train_args['dfl']}\")\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"MONITOR CONSOLE OUTPUT FOR REAL-TIME PROGRESS\")\n",
    "print(\"Metrics will be logged to W&B after completion\")\n",
    "print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "# Train model\n",
    "try:\n",
    "    results = model.train(**train_args)\n",
    "except Exception as e:\n",
    "    print(f\"Training error: {e}\")\n",
    "    if config.use_wandb:\n",
    "        wandb.alert(title=\"Training Failed\", text=str(e))\n",
    "    raise\n",
    "\n",
    "# Calculate training time\n",
    "training_end_time = time.time()\n",
    "total_training_time = training_end_time - training_start_time\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"TRAINING COMPLETED\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Total training time: {total_training_time/60:.2f} minutes\")\n",
    "print(f\"Average time per epoch: {total_training_time/config.epochs:.2f} seconds\")\n",
    "\n",
    "# Store metrics for later use\n",
    "training_metrics['total_training_time'] = total_training_time\n",
    "training_metrics['end_time'] = training_end_time\n",
    "\n",
    "# Get paths\n",
    "best_model_path = Path(results.save_dir) / 'weights' / 'best.pt'\n",
    "last_model_path = Path(results.save_dir) / 'weights' / 'last.pt'\n",
    "results_csv = Path(results.save_dir) / 'results.csv'\n",
    "\n",
    "# LOG COMPREHENSIVE TRAINING DATA TO W&B\n",
    "if config.use_wandb and results_csv.exists():\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"LOGGING TRAINING METRICS TO W&B\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # 1. Read and log training curves\n",
    "    df = pd.read_csv(results_csv)\n",
    "    df.columns = [col.strip() for col in df.columns]  # Clean column names\n",
    "    \n",
    "    print(f\"Logging {len(df)} epochs of training data...\")\n",
    "    \n",
    "    # Log each epoch's metrics\n",
    "    for idx, row in df.iterrows():\n",
    "        epoch = int(row['epoch'])\n",
    "        \n",
    "        metrics = {\n",
    "            # Training losses\n",
    "            'train/box_loss': float(row['train/box_loss']) if 'train/box_loss' in row else None,\n",
    "            'train/cls_loss': float(row['train/cls_loss']) if 'train/cls_loss' in row else None,\n",
    "            'train/dfl_loss': float(row['train/dfl_loss']) if 'train/dfl_loss' in row else None,\n",
    "            \n",
    "            # Validation losses\n",
    "            'val/box_loss': float(row['val/box_loss']) if 'val/box_loss' in row else None,\n",
    "            'val/cls_loss': float(row['val/cls_loss']) if 'val/cls_loss' in row else None,\n",
    "            'val/dfl_loss': float(row['val/dfl_loss']) if 'val/dfl_loss' in row else None,\n",
    "            \n",
    "            # Performance metrics\n",
    "            'metrics/precision': float(row['metrics/precision(B)']) if 'metrics/precision(B)' in row else None,\n",
    "            'metrics/recall': float(row['metrics/recall(B)']) if 'metrics/recall(B)' in row else None,\n",
    "            'metrics/mAP50': float(row['metrics/mAP50(B)']) if 'metrics/mAP50(B)' in row else None,\n",
    "            'metrics/mAP50-95': float(row['metrics/mAP50-95(B)']) if 'metrics/mAP50-95(B)' in row else None,\n",
    "        }\n",
    "        \n",
    "        # Remove None values\n",
    "        metrics = {k: v for k, v in metrics.items() if v is not None}\n",
    "        \n",
    "        # Log with epoch as step\n",
    "        wandb.log(metrics, step=epoch)\n",
    "    \n",
    "    # 2. Find and log best epoch\n",
    "    best_map50_idx = df['metrics/mAP50(B)'].idxmax()\n",
    "    best_epoch = int(df.loc[best_map50_idx, 'epoch'])\n",
    "    best_map50 = float(df.loc[best_map50_idx, 'metrics/mAP50(B)'])\n",
    "    \n",
    "    training_metrics['best_metrics'] = {\n",
    "        'epoch': best_epoch,\n",
    "        'mAP50': best_map50\n",
    "    }\n",
    "    \n",
    "    # 3. Log training plots if they exist\n",
    "    plots_path = Path(results.save_dir)\n",
    "    plot_files = ['results.png', 'confusion_matrix.png', 'F1_curve.png', \n",
    "                  'P_curve.png', 'R_curve.png', 'PR_curve.png']\n",
    "    \n",
    "    for plot_file in plot_files:\n",
    "        plot_path = plots_path / plot_file\n",
    "        if plot_path.exists():\n",
    "            wandb.log({f\"training_plots/{plot_file.split('.')[0]}\": wandb.Image(str(plot_path))})\n",
    "            print(f\"  ✓ Logged {plot_file}\")\n",
    "    \n",
    "    # 4. Log final summary metrics\n",
    "    wandb.log({\n",
    "        'summary/total_training_time_min': total_training_time/60,\n",
    "        'summary/avg_epoch_time_sec': total_training_time/config.epochs,\n",
    "        'summary/best_epoch': best_epoch,\n",
    "        'summary/best_mAP50': best_map50,\n",
    "        'summary/final_mAP50': float(df.iloc[-1]['metrics/mAP50(B)']),\n",
    "        'summary/training_completed': True,\n",
    "    })\n",
    "    \n",
    "    # Update run summary\n",
    "    wandb.run.summary.update({\n",
    "        'training_completed': True,\n",
    "        'total_epochs': config.epochs,\n",
    "        'best_epoch': best_epoch,\n",
    "        'best_mAP50': best_map50,\n",
    "        'total_training_time_minutes': total_training_time/60\n",
    "    })\n",
    "    \n",
    "    # Log strategy-specific metrics\n",
    "    strategy.log_strategy_metrics({})\n",
    "    \n",
    "    print(f\"\\n✓ Successfully logged all training metrics to W&B\")\n",
    "    print(f\"✓ Best mAP50: {best_map50:.4f} at epoch {best_epoch}\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\n⚠️ No results.csv found or W&B disabled - skipping metric logging\")\n",
    "\n",
    "print(f\"\\nModel weights saved:\")\n",
    "print(f\"  Best: {best_model_path}\")\n",
    "print(f\"  Last: {last_model_path}\")\n",
    "\n",
    "# Save paths for next cells\n",
    "model_paths = {\n",
    "    'best': best_model_path,\n",
    "    'last': last_model_path,\n",
    "    'results_dir': results.save_dir\n",
    "}\n",
    "\n",
    "print(f\"\\nReady for evaluation!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb7d96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Initialize Evaluator\n",
    "print(\"=\"*70)\n",
    "print(\"INITIALIZING COMPREHENSIVE EVALUATOR\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "\n",
    "evaluator = ComprehensiveEvaluator(\n",
    "    model_path=best_model_path,\n",
    "    dataset_path=yolo_path,\n",
    "    config=config,\n",
    "    output_dir=Path('../results') / experiment_name / 'evaluation'\n",
    ")\n",
    "\n",
    "print(f\"✓ Model loaded: {config.model_name}\")\n",
    "print(f\"✓ Dataset: {config.dataset.upper()}\")\n",
    "print(f\"✓ Task: {config.task}\")\n",
    "print(f\"✓ Output directory created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26aeb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Global Metrics - Validation Set\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"VALIDATION SET - GLOBAL METRICS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "val_results = evaluator.run_full_evaluation(split='val')\n",
    "\n",
    "# Create formatted table\n",
    "val_global_data = []\n",
    "val_global_data.append(['mAP@0.5', f\"{val_results['global']['mAP50']:.4f}\"])\n",
    "val_global_data.append(['mAP@[0.5:0.95]', f\"{val_results['global']['mAP50-95']:.4f}\"])\n",
    "val_global_data.append(['Mean Precision', f\"{val_results['global'].get('precision', 0):.4f}\"])\n",
    "val_global_data.append(['Mean Recall', f\"{val_results['global'].get('recall', 0):.4f}\"])\n",
    "\n",
    "print(\"\\n\" + tabulate(val_global_data, \n",
    "                      headers=['Metric', 'Value'],\n",
    "                      tablefmt='fancy_grid',\n",
    "                      numalign='right'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e86a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Global Metrics - Test Set\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TEST SET - GLOBAL METRICS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "test_results = evaluator.run_full_evaluation(split='test')\n",
    "\n",
    "# Create formatted table\n",
    "test_global_data = []\n",
    "test_global_data.append(['mAP@0.5', f\"{test_results['global']['mAP50']:.4f}\"])\n",
    "test_global_data.append(['mAP@[0.5:0.95]', f\"{test_results['global']['mAP50-95']:.4f}\"])\n",
    "test_global_data.append(['Mean Precision', f\"{test_results['global'].get('precision', 0):.4f}\"])\n",
    "test_global_data.append(['Mean Recall', f\"{test_results['global'].get('recall', 0):.4f}\"])\n",
    "\n",
    "print(\"\\n\" + tabulate(test_global_data, \n",
    "                      headers=['Metric', 'Value'],\n",
    "                      tablefmt='fancy_grid',\n",
    "                      numalign='right'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901cf820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13A: Per-Class Performance Analysis - VALIDATION SET\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"VALIDATION SET - PER-CLASS PERFORMANCE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "val_class_data = []\n",
    "for class_name in config.get_class_names().values():\n",
    "    metrics = val_results['per_class'].get(class_name, {})\n",
    "    val_class_data.append([\n",
    "        class_name,\n",
    "        f\"{metrics.get('precision', 0):.4f}\",\n",
    "        f\"{metrics.get('recall', 0):.4f}\",\n",
    "        f\"{metrics.get('f1_score', 0):.4f}\",\n",
    "        metrics.get('tp', 0),\n",
    "        metrics.get('fp', 0),\n",
    "        metrics.get('fn', 0),\n",
    "        metrics.get('support', 0)\n",
    "    ])\n",
    "\n",
    "print(\"\\n\" + tabulate(val_class_data,\n",
    "                      headers=['Class', 'Precision', 'Recall', 'F1-Score', 'TP', 'FP', 'FN', 'Support'],\n",
    "                      tablefmt='fancy_grid',\n",
    "                      numalign='right'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e7fc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8.5: Inference Time Tracking\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"INFERENCE TIME ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Test inference speed on different batch sizes\n",
    "test_images = list((yolo_path / 'test' / 'images').glob('*.jpg'))[:100]\n",
    "\n",
    "inference_results = {}\n",
    "batch_sizes = [1, 8, 16, 32] if torch.cuda.is_available() else [1, 4, 8]\n",
    "\n",
    "model_eval = YOLO(best_model_path)\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    times = []\n",
    "    \n",
    "    for i in range(0, min(len(test_images), 32), batch_size):\n",
    "        batch = test_images[i:i+batch_size]\n",
    "        \n",
    "        start_time = time.time()\n",
    "        results = model_eval.predict(batch, conf=0.5, iou=0.5, verbose=False)\n",
    "        end_time = time.time()\n",
    "        \n",
    "        batch_time = end_time - start_time\n",
    "        times.append(batch_time / len(batch))  # Time per image\n",
    "    \n",
    "    avg_time = np.mean(times) * 1000  # Convert to ms\n",
    "    std_time = np.std(times) * 1000\n",
    "    \n",
    "    inference_results[f'batch_{batch_size}'] = {\n",
    "        'mean_ms': avg_time,\n",
    "        'std_ms': std_time,\n",
    "        'fps': 1000/avg_time if avg_time > 0 else 0\n",
    "    }\n",
    "    \n",
    "    print(f\"Batch size {batch_size}: {avg_time:.2f}±{std_time:.2f} ms/image ({1000/avg_time:.1f} FPS)\")\n",
    "\n",
    "# Store for later - W&B logging will happen in Cell 19\n",
    "training_metrics['inference_results'] = inference_results\n",
    "\n",
    "print(f\"\\n✓ Inference results stored for final logging\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f3fc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13B: Per-Class Performance Analysis - TEST SET\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TEST SET - PER-CLASS PERFORMANCE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "test_class_data = []\n",
    "for class_name in config.get_class_names().values():\n",
    "    metrics = test_results['per_class'].get(class_name, {})\n",
    "    test_class_data.append([\n",
    "        class_name,\n",
    "        f\"{metrics.get('precision', 0):.4f}\",\n",
    "        f\"{metrics.get('recall', 0):.4f}\",\n",
    "        f\"{metrics.get('f1_score', 0):.4f}\",\n",
    "        metrics.get('tp', 0),\n",
    "        metrics.get('fp', 0),\n",
    "        metrics.get('fn', 0),\n",
    "        metrics.get('support', 0)\n",
    "    ])\n",
    "\n",
    "print(\"\\n\" + tabulate(test_class_data,\n",
    "                      headers=['Class', 'Precision', 'Recall', 'F1-Score', 'TP', 'FP', 'FN', 'Support'],\n",
    "                      tablefmt='fancy_grid',\n",
    "                      numalign='right'))\n",
    "\n",
    "# Visualization comparing Val vs Test\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "fig.suptitle('Per-Class Performance: Validation vs Test', fontsize=14, fontweight='bold')\n",
    "\n",
    "classes = list(config.get_class_names().values())\n",
    "x = np.arange(len(classes))\n",
    "width = 0.35\n",
    "\n",
    "# Precision comparison\n",
    "val_prec = [val_results['per_class'].get(c, {}).get('precision', 0) for c in classes]\n",
    "test_prec = [test_results['per_class'].get(c, {}).get('precision', 0) for c in classes]\n",
    "\n",
    "axes[0].bar(x - width/2, val_prec, width, label='Validation', color='skyblue', edgecolor='navy')\n",
    "axes[0].bar(x + width/2, test_prec, width, label='Test', color='lightcoral', edgecolor='darkred')\n",
    "axes[0].set_ylabel('Precision', fontsize=11)\n",
    "axes[0].set_title('Precision', fontsize=12)\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(classes)\n",
    "axes[0].legend()\n",
    "axes[0].set_ylim(0, 1.05)\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Recall comparison\n",
    "val_rec = [val_results['per_class'].get(c, {}).get('recall', 0) for c in classes]\n",
    "test_rec = [test_results['per_class'].get(c, {}).get('recall', 0) for c in classes]\n",
    "\n",
    "axes[1].bar(x - width/2, val_rec, width, label='Validation', color='skyblue', edgecolor='navy')\n",
    "axes[1].bar(x + width/2, test_rec, width, label='Test', color='lightcoral', edgecolor='darkred')\n",
    "axes[1].set_ylabel('Recall', fontsize=11)\n",
    "axes[1].set_title('Recall', fontsize=12)\n",
    "axes[1].set_xticks(x)\n",
    "axes[1].set_xticklabels(classes)\n",
    "axes[1].legend()\n",
    "axes[1].set_ylim(0, 1.05)\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# F1 comparison\n",
    "val_f1 = [val_results['per_class'].get(c, {}).get('f1_score', 0) for c in classes]\n",
    "test_f1 = [test_results['per_class'].get(c, {}).get('f1_score', 0) for c in classes]\n",
    "\n",
    "axes[2].bar(x - width/2, val_f1, width, label='Validation', color='skyblue', edgecolor='navy')\n",
    "axes[2].bar(x + width/2, test_f1, width, label='Test', color='lightcoral', edgecolor='darkred')\n",
    "axes[2].set_ylabel('F1-Score', fontsize=11)\n",
    "axes[2].set_title('F1-Score', fontsize=12)\n",
    "axes[2].set_xticks(x)\n",
    "axes[2].set_xticklabels(classes)\n",
    "axes[2].legend()\n",
    "axes[2].set_ylim(0, 1.05)\n",
    "axes[2].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "save_path = Path('../results') / experiment_name / 'per_class_comparison.png'\n",
    "plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7d3b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 14: Recall Variability Analysis - UPDATED WITH WANDB TABLE FIX\n",
    "if config.task == 'binary':\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"RECALL VARIABILITY ANALYSIS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Collect per-image data\n",
    "    img_dir = yolo_path / 'test' / \"images\"\n",
    "    lbl_dir = yolo_path / 'test' / \"labels\"\n",
    "    \n",
    "    infected_ratios = []\n",
    "    infected_recalls = []\n",
    "    image_names = []\n",
    "    \n",
    "    img_files = list(img_dir.glob('*.jpg')) + list(img_dir.glob('*.png'))\n",
    "    \n",
    "    for img_path in tqdm(img_files, desc=\"Analyzing recall variability\"):\n",
    "        label_path = lbl_dir / (img_path.stem + '.txt')\n",
    "        \n",
    "        if not label_path.exists():\n",
    "            continue\n",
    "            \n",
    "        # Count GT boxes\n",
    "        infected_gt = 0\n",
    "        uninfected_gt = 0\n",
    "        infected_boxes = []\n",
    "        \n",
    "        img = Image.open(img_path)\n",
    "        img_width, img_height = img.size\n",
    "        \n",
    "        with open(label_path, 'r') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) >= 5:\n",
    "                    class_id = int(parts[0])\n",
    "                    if class_id == 1:  # Infected\n",
    "                        infected_gt += 1\n",
    "                        x_center = float(parts[1]) * img_width\n",
    "                        y_center = float(parts[2]) * img_height\n",
    "                        width = float(parts[3]) * img_width\n",
    "                        height = float(parts[4]) * img_height\n",
    "                        infected_boxes.append([\n",
    "                            x_center - width/2, y_center - height/2,\n",
    "                            x_center + width/2, y_center + height/2\n",
    "                        ])\n",
    "                    else:\n",
    "                        uninfected_gt += 1\n",
    "        \n",
    "        total_gt = infected_gt + uninfected_gt\n",
    "        if total_gt == 0 or infected_gt == 0:\n",
    "            continue\n",
    "        \n",
    "        # Calculate infected ratio\n",
    "        infected_ratio = (infected_gt / total_gt) * 100\n",
    "        \n",
    "        # Get predictions and calculate recall\n",
    "        results = evaluator.model.predict(img_path, conf=0.5, iou=0.5, verbose=False)[0]\n",
    "        \n",
    "        infected_tp = 0\n",
    "        if results.boxes is not None:\n",
    "            for box in results.boxes:\n",
    "                if int(box.cls.item()) == 1:  # Infected prediction\n",
    "                    pred_box = box.xyxy[0].tolist()\n",
    "                    # Check if matches any infected GT\n",
    "                    for gt_box in infected_boxes:\n",
    "                        iou = evaluator._compute_iou(pred_box, gt_box)\n",
    "                        if iou > 0.5:\n",
    "                            infected_tp += 1\n",
    "                            break\n",
    "        \n",
    "        infected_recall = infected_tp / infected_gt\n",
    "        infected_ratios.append(infected_ratio)\n",
    "        infected_recalls.append(infected_recall)\n",
    "        image_names.append(img_path.stem)\n",
    "    \n",
    "    # SAVE DATA\n",
    "    import pandas as pd\n",
    "    \n",
    "    # Create DataFrame\n",
    "    recall_df = pd.DataFrame({\n",
    "        'image_id': image_names,\n",
    "        'infected_ratio_percent': infected_ratios,\n",
    "        'infected_recall': infected_recalls\n",
    "    })\n",
    "    \n",
    "    # Save to CSV\n",
    "    csv_path = Path('../results') / experiment_name / 'recall_variability_data.csv'\n",
    "    recall_df.to_csv(csv_path, index=False)\n",
    "    print(f\"Saved recall data to: {csv_path}\")\n",
    "    \n",
    "    # CRITICAL FIX: Store data for W&B properly\n",
    "    test_results['recall_variability'] = {\n",
    "        'wandb_table': recall_df.to_dict('records'),  # This creates the wandb_table\n",
    "        'summary': {\n",
    "            'n_images': len(infected_ratios),\n",
    "            'mean_recall': float(np.mean(infected_recalls)) if infected_recalls else 0,\n",
    "            'std_recall': float(np.std(infected_recalls)) if infected_recalls else 0,\n",
    "            'min_recall': float(np.min(infected_recalls)) if infected_recalls else 0,\n",
    "            'max_recall': float(np.max(infected_recalls)) if infected_recalls else 0\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # [REST OF VISUALIZATION CODE REMAINS THE SAME - scatter plot, etc.]\n",
    "    # Create scatter plot\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    \n",
    "    scatter = ax.scatter(infected_ratios, infected_recalls, \n",
    "                        c=infected_recalls, cmap='RdYlGn', \n",
    "                        s=50, alpha=0.6, edgecolors='black', linewidth=0.5)\n",
    "    \n",
    "    cbar = plt.colorbar(scatter, ax=ax)\n",
    "    cbar.set_label('Infected Cell Recall', fontsize=11)\n",
    "    \n",
    "    if len(infected_ratios) > 2:\n",
    "        z = np.polyfit(infected_ratios, infected_recalls, 2)\n",
    "        p = np.poly1d(z)\n",
    "        x_trend = np.linspace(0, max(infected_ratios), 100)\n",
    "        ax.plot(x_trend, p(x_trend), \"b--\", alpha=0.5, linewidth=2, label='Trend')\n",
    "    \n",
    "    ax.axvspan(0, 1, alpha=0.1, color='red', label='0-1% (Critical)')\n",
    "    ax.axvspan(1, 3, alpha=0.1, color='orange', label='1-3% (Low density)')\n",
    "    ax.axvspan(3, 5, alpha=0.1, color='yellow', label='3-5% (Medium)')\n",
    "    ax.axvspan(5, max(infected_ratios) if infected_ratios else 10, \n",
    "               alpha=0.1, color='green', label='>5% (High density)')\n",
    "    \n",
    "    ax.set_xlabel('Infected Cell Ratio (%)', fontsize=12)\n",
    "    ax.set_ylabel('Infected Cell Recall', fontsize=12)\n",
    "    ax.set_title('Recall Variability: Per-Image Infected Cell Detection Performance', \n",
    "                fontsize=14, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3, linestyle='--')\n",
    "    ax.set_ylim(-0.05, 1.05)\n",
    "    ax.set_xlim(0, max(infected_ratios) * 1.1 if infected_ratios else 10)\n",
    "    \n",
    "    ax.text(0.02, 0.98, f'Total Images: {len(infected_ratios)}\\n'\n",
    "                        f'Mean Recall: {np.mean(infected_recalls):.3f}\\n'\n",
    "                        f'Std Recall: {np.std(infected_recalls):.3f}',\n",
    "            transform=ax.transAxes, fontsize=10,\n",
    "            verticalalignment='top',\n",
    "            bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    \n",
    "    ax.legend(loc='lower right', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    save_path = Path('../results') / experiment_name / 'recall_variability.png'\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nAnalyzed {len(infected_ratios)} images with infected cells\")\n",
    "    print(f\"Mean recall: {np.mean(infected_recalls):.3f} ± {np.std(infected_recalls):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdaef475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 14: Prevalence-Stratified Analysis - FINAL UPDATED VERSION\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tabulate import tabulate\n",
    "from pathlib import Path\n",
    "\n",
    "if config.task == 'binary':\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"PREVALENCE-STRATIFIED ANALYSIS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    stratified = test_results['stratified']\n",
    "    \n",
    "    # Using established bins from previous work\n",
    "    bins = ['0-1%', '1-3%', '3-5%', '>5%']\n",
    "    \n",
    "    # Display results table\n",
    "    strat_data = []\n",
    "    for bin_name in bins:\n",
    "        stats = stratified[bin_name]\n",
    "        strat_data.append([\n",
    "            bin_name,\n",
    "            f\"{stats['mean_recall']:.3f}\",\n",
    "            f\"{stats['std_recall']:.3f}\",\n",
    "            stats['count']\n",
    "        ])\n",
    "    \n",
    "    print(\"\\n\" + tabulate(strat_data,\n",
    "                          headers=['Parasitemia Level', 'Mean Recall', 'Std Dev', 'N Images'],\n",
    "                          tablefmt='fancy_grid',\n",
    "                          numalign='right'))\n",
    "    \n",
    "    # Visualization\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    fig.suptitle('Performance Across Parasitemia Levels', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    recalls = [stratified[b]['mean_recall'] for b in bins]\n",
    "    stds = [stratified[b]['std_recall'] for b in bins]\n",
    "    counts = [stratified[b]['count'] for b in bins]\n",
    "    \n",
    "    # Color gradient from red (critical) to green (less critical)\n",
    "    colors = ['#B71C1C', '#FF6F00', '#FDD835', '#43A047']\n",
    "    \n",
    "    # 1. Bar chart with error bars - ALL bins shown equally\n",
    "    bars = []\n",
    "    for i, (bin_name, recall, std, count) in enumerate(zip(bins, recalls, stds, counts)):\n",
    "        bar = ax1.bar(i, recall, yerr=std if count > 0 else 0, \n",
    "                      capsize=5,\n",
    "                      color=colors[i], \n",
    "                      edgecolor='black', \n",
    "                      linewidth=1.5,\n",
    "                      alpha=0.8)  # Consistent opacity for all\n",
    "        bars.append(bar)\n",
    "    \n",
    "    ax1.set_xticks(range(len(bins)))\n",
    "    ax1.set_xticklabels(bins)\n",
    "    ax1.set_xlabel('Parasitemia Level (%)', fontsize=12)\n",
    "    ax1.set_ylabel('Mean Infected Cell Recall', fontsize=12)\n",
    "    ax1.set_title('Recall by Parasitemia Level', fontsize=12)\n",
    "    ax1.set_ylim(0, 1.05)\n",
    "    ax1.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "    \n",
    "    # Add clinical threshold\n",
    "    ax1.axhline(y=0.8, color='green', linestyle=':', linewidth=1.5, alpha=0.7)\n",
    "    ax1.text(3.5, 0.82, 'Target: 0.8', fontsize=9, color='green')\n",
    "    \n",
    "    # Add sample counts on bars - no warnings\n",
    "    for i, (bar_container, count, recall) in enumerate(zip(bars, counts, recalls)):\n",
    "        bar = bar_container[0]\n",
    "        height = bar.get_height()\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2, height + 0.02,\n",
    "                f'n={count}', ha='center', va='bottom', \n",
    "                fontsize=9, fontweight='bold')\n",
    "    \n",
    "    # 2. Line plot with confidence intervals - ALL points shown\n",
    "    x_pos = np.arange(len(bins))\n",
    "    \n",
    "    # Plot trend line with all points equally visible\n",
    "    ax2.plot(x_pos, recalls, 'o-', markersize=10, linewidth=2.5, \n",
    "             color='#D32F2F', markeredgecolor='black', markeredgewidth=1,\n",
    "             label='Mean Recall')\n",
    "    \n",
    "    # Add error bars for all points\n",
    "    ax2.errorbar(x_pos, recalls, yerr=stds, fmt='none', \n",
    "                ecolor='#D32F2F', alpha=0.3, capsize=5, capthick=2)\n",
    "    \n",
    "    # Fill confidence interval\n",
    "    ax2.fill_between(x_pos,\n",
    "                     [r-s for r,s in zip(recalls, stds)],\n",
    "                     [r+s for r,s in zip(recalls, stds)],\n",
    "                     alpha=0.15, color='#D32F2F')\n",
    "    \n",
    "    ax2.set_xticks(x_pos)\n",
    "    ax2.set_xticklabels(bins)\n",
    "    ax2.set_xlabel('Parasitemia Level (%)', fontsize=12)\n",
    "    ax2.set_ylabel('Mean Infected Cell Recall', fontsize=12)\n",
    "    ax2.set_title('Performance Trend Across Parasitemia Levels', fontsize=12)\n",
    "    ax2.set_ylim(0, 1.05)\n",
    "    ax2.grid(True, alpha=0.3, linestyle='--')\n",
    "    \n",
    "    # Add clinical thresholds\n",
    "    ax2.axhline(y=0.8, color='green', linestyle=':', linewidth=1.5, alpha=0.7, label='Clinical Target')\n",
    "    ax2.axhline(y=0.5, color='orange', linestyle=':', linewidth=1.5, alpha=0.7, label='Minimum Acceptable')\n",
    "    \n",
    "    ax2.legend(loc='lower right', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    save_path = Path('../results') / experiment_name / 'stratified_analysis.png'\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Clinical findings summary - no reliability warnings\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"CLINICAL PERFORMANCE SUMMARY\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Report all parasitemia levels\n",
    "    print(\"Detection performance by parasitemia level:\")\n",
    "    for bin_name in bins:\n",
    "        recall = stratified[bin_name]['mean_recall']\n",
    "        std = stratified[bin_name]['std_recall']\n",
    "        count = stratified[bin_name]['count']\n",
    "        \n",
    "        if bin_name == '0-1%':\n",
    "            clinical_note = \" [Ultra-low density - critical for early detection]\"\n",
    "        elif bin_name == '1-3%':\n",
    "            clinical_note = \" [Low density - most common in clinical practice]\"\n",
    "        elif bin_name == '3-5%':\n",
    "            clinical_note = \" [Medium density]\"\n",
    "        else:\n",
    "            clinical_note = \" [Higher density - easily detectable]\"\n",
    "        \n",
    "        print(f\"  {bin_name}: {recall:.3f} ± {std:.3f} (n={count}){clinical_note}\")\n",
    "    \n",
    "    # Performance comparison\n",
    "    if stratified['>5%']['mean_recall'] > 0 and stratified['1-3%']['mean_recall'] > 0:\n",
    "        sparse_recall = stratified['1-3%']['mean_recall']\n",
    "        dense_recall = stratified['>5%']['mean_recall']\n",
    "        drop = (dense_recall - sparse_recall) / dense_recall * 100\n",
    "        print(f\"\\nPerformance drop from high (>5%) to low (1-3%) density: {drop:.1f}%\")\n",
    "    \n",
    "    # Critical low-parasitemia detection (<=3%)\n",
    "    low_parasitemia_recalls = []\n",
    "    low_parasitemia_counts = 0\n",
    "    \n",
    "    for bin_name in ['0-1%', '1-3%']:\n",
    "        if stratified[bin_name]['count'] > 0:\n",
    "            low_parasitemia_recalls.extend([stratified[bin_name]['mean_recall']] * stratified[bin_name]['count'])\n",
    "            low_parasitemia_counts += stratified[bin_name]['count']\n",
    "    \n",
    "    if low_parasitemia_recalls:\n",
    "        mean_low_parasitemia = np.mean(low_parasitemia_recalls)\n",
    "        print(f\"\\nCombined performance for ≤3% parasitemia: {mean_low_parasitemia:.3f}\")\n",
    "        print(f\"  Total images in this critical range: {low_parasitemia_counts}\")\n",
    "        \n",
    "        if mean_low_parasitemia >= 0.8:\n",
    "            print(\"  ✓ Meets clinical target for low-density detection\")\n",
    "        else:\n",
    "            deficit = 0.8 - mean_low_parasitemia\n",
    "            print(f\"  → {deficit:.3f} below clinical target of 0.8\")\n",
    "    \n",
    "    # Sample distribution analysis\n",
    "    print(f\"\\nTest set parasitemia distribution:\")\n",
    "    total_samples = sum(counts)\n",
    "    for bin_name, count in zip(bins, counts):\n",
    "        percentage = (count / total_samples * 100) if total_samples > 0 else 0\n",
    "        print(f\"  {bin_name}: {count} images ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Note about sparse detection challenges\n",
    "    if stratified['0-1%']['count'] > 0 or stratified['1-3%']['count'] > 0:\n",
    "        print(\"\\nNote: Low parasitemia (<3%) represents the most challenging\")\n",
    "        print(\"      and clinically important detection scenario.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b3d029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cell: Complete Reset and Reload\n",
    "# import importlib\n",
    "# import sys\n",
    "# from pathlib import Path\n",
    "\n",
    "# # 1. Clear ALL cached modules\n",
    "# modules_to_clear = [k for k in sys.modules.keys() if 'evaluation' in k or 'evaluator' in k]\n",
    "# for module in modules_to_clear:\n",
    "#     del sys.modules[module]\n",
    "\n",
    "# # 2. Also clear any .pyc files (compiled Python)\n",
    "# import os\n",
    "# import shutil\n",
    "# pyc_path = Path('src/evaluation/__pycache__')\n",
    "# if pyc_path.exists():\n",
    "#     shutil.rmtree(pyc_path)\n",
    "#     print(f\"Cleared {pyc_path}\")\n",
    "\n",
    "# # 3. Force reimport\n",
    "# from src.evaluation.evaluator import ComprehensiveEvaluator\n",
    "\n",
    "# # 4. CREATE A NEW EVALUATOR OBJECT (Critical!)\n",
    "# print(\"Creating NEW evaluator object with updated code...\")\n",
    "# evaluator = ComprehensiveEvaluator(\n",
    "#     model_path=best_model_path,\n",
    "#     dataset_path=yolo_path,\n",
    "#     config=config,\n",
    "#     output_dir=Path('../results') / experiment_name / 'evaluation'\n",
    "# )\n",
    "\n",
    "# # 5. Now run evaluation with the NEW evaluator\n",
    "# print(\"Running evaluation with updated evaluator...\")\n",
    "# test_results = evaluator.run_full_evaluation(split='test')\n",
    "\n",
    "# # 6. Verify the curve data is there\n",
    "# pr_analysis = test_results.get('pr_analysis', {})\n",
    "# for class_name in pr_analysis:\n",
    "#     has_curves = 'precision_values' in pr_analysis[class_name]\n",
    "#     print(f\"{class_name}: Curves stored = {has_curves}\")\n",
    "#     if has_curves:\n",
    "#         print(f\"  - Points in curve: {len(pr_analysis[class_name]['precision_values'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8da9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 15: Enhanced Precision-Recall Curve Analysis with Professional Visualization\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tabulate import tabulate\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PRECISION-RECALL ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "pr_analysis = test_results['pr_analysis']\n",
    "\n",
    "# Table\n",
    "pr_data = []\n",
    "for class_name, pr_stats in pr_analysis.items():\n",
    "    pr_data.append([\n",
    "        class_name,\n",
    "        f\"{pr_stats.get('ap', 0):.3f}\",\n",
    "        f\"{pr_stats.get('optimal_threshold', 0):.3f}\",\n",
    "        f\"{pr_stats.get('precision_at_optimal', 0):.3f}\",\n",
    "        f\"{pr_stats.get('recall_at_optimal', 0):.3f}\",\n",
    "        f\"{pr_stats.get('max_f1', 0):.3f}\"\n",
    "    ])\n",
    "\n",
    "print(\"\\n\" + tabulate(pr_data,\n",
    "                      headers=['Class', 'AP', 'Optimal Thresh', 'Precision', 'Recall', 'Max F1'],\n",
    "                      tablefmt='fancy_grid',\n",
    "                      numalign='right'))\n",
    "\n",
    "# Create professional visualization\n",
    "fig = plt.figure(figsize=(18, 10))\n",
    "gs = fig.add_gridspec(2, 3, hspace=0.3, wspace=0.3)\n",
    "\n",
    "# Color scheme\n",
    "colors = {'Uninfected': '#2E7D32', 'Infected': '#C62828'}  # Professional green/red\n",
    "markers = {'Uninfected': 'o', 'Infected': 's'}\n",
    "\n",
    "# 1. Main P-R Curves (Top row, spans 2 columns)\n",
    "ax1 = fig.add_subplot(gs[0, :2])\n",
    "ax1.set_title('Precision-Recall Curves', fontsize=14, fontweight='bold', pad=15)\n",
    "\n",
    "# Plot curves with better styling\n",
    "for class_name, pr_stats in pr_analysis.items():\n",
    "    if 'precision_values' in pr_stats and 'recall_values' in pr_stats:\n",
    "        # Get data\n",
    "        recall = np.array(pr_stats['recall_values'])\n",
    "        precision = np.array(pr_stats['precision_values'])\n",
    "        \n",
    "        # Plot main curve\n",
    "        ax1.plot(recall, precision,\n",
    "                color=colors.get(class_name, '#1976D2'),\n",
    "                linewidth=2.5,\n",
    "                label=f'{class_name} (AP = {pr_stats[\"ap\"]:.3f})',\n",
    "                alpha=0.9)\n",
    "        \n",
    "        # Add shaded area under curve\n",
    "        ax1.fill_between(recall, 0, precision,\n",
    "                         alpha=0.15,\n",
    "                         color=colors.get(class_name, '#1976D2'))\n",
    "        \n",
    "        # Mark optimal operating point\n",
    "        if 'recall_at_optimal' in pr_stats and 'precision_at_optimal' in pr_stats:\n",
    "            ax1.plot(pr_stats['recall_at_optimal'], \n",
    "                    pr_stats['precision_at_optimal'],\n",
    "                    marker=markers.get(class_name, 'o'),\n",
    "                    markersize=12,\n",
    "                    color=colors.get(class_name, '#1976D2'),\n",
    "                    markeredgecolor='white',\n",
    "                    markeredgewidth=2,\n",
    "                    zorder=5)\n",
    "            \n",
    "            # Add annotation for optimal point\n",
    "            ax1.annotate(f'F1={pr_stats[\"max_f1\"]:.2f}',\n",
    "                        xy=(pr_stats['recall_at_optimal'], pr_stats['precision_at_optimal']),\n",
    "                        xytext=(10, -10), textcoords='offset points',\n",
    "                        fontsize=9, color=colors.get(class_name, '#1976D2'),\n",
    "                        bbox=dict(boxstyle='round,pad=0.3', facecolor='white', \n",
    "                                 edgecolor=colors.get(class_name, '#1976D2'), alpha=0.8))\n",
    "\n",
    "# Add reference lines\n",
    "ax1.axhline(y=0.5, color='gray', linestyle=':', alpha=0.3, linewidth=1)\n",
    "ax1.axvline(x=0.5, color='gray', linestyle=':', alpha=0.3, linewidth=1)\n",
    "ax1.plot([0, 1], [1, 0], 'k--', alpha=0.2, linewidth=1, label='Random')\n",
    "\n",
    "# Formatting\n",
    "ax1.set_xlabel('Recall (Sensitivity)', fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel('Precision (PPV)', fontsize=12, fontweight='bold')\n",
    "ax1.set_xlim([-0.02, 1.02])\n",
    "ax1.set_ylim([-0.02, 1.02])\n",
    "ax1.grid(True, alpha=0.2, linestyle='-', linewidth=0.5)\n",
    "ax1.legend(loc='lower left', fontsize=11, frameon=True, fancybox=True, \n",
    "          shadow=True, framealpha=0.95)\n",
    "\n",
    "# Add performance zones\n",
    "ax1.text(0.85, 0.85, 'Excellent', fontsize=9, alpha=0.3, ha='center')\n",
    "ax1.text(0.5, 0.5, 'Good', fontsize=9, alpha=0.3, ha='center')\n",
    "ax1.text(0.15, 0.15, 'Poor', fontsize=9, alpha=0.3, ha='center')\n",
    "\n",
    "# 2. AP Bar Chart (Top right)\n",
    "ax2 = fig.add_subplot(gs[0, 2])\n",
    "classes = list(pr_analysis.keys())\n",
    "aps = [pr_analysis[c]['ap'] for c in classes]\n",
    "\n",
    "bars = ax2.barh(classes, aps, color=[colors.get(c, '#1976D2') for c in classes],\n",
    "                edgecolor='white', linewidth=2, alpha=0.85, height=0.6)\n",
    "\n",
    "# Add value labels\n",
    "for i, (bar, ap) in enumerate(zip(bars, aps)):\n",
    "    ax2.text(ap + 0.02, bar.get_y() + bar.get_height()/2,\n",
    "             f'{ap:.3f}', va='center', fontsize=11, fontweight='bold')\n",
    "\n",
    "ax2.set_xlim([0, 1.05])\n",
    "ax2.set_xlabel('Average Precision', fontsize=11, fontweight='bold')\n",
    "ax2.set_title('Average Precision Score', fontsize=12, fontweight='bold')\n",
    "ax2.axvline(x=0.8, color='green', linestyle=':', alpha=0.4, linewidth=1.5)\n",
    "ax2.text(0.8, -0.5, 'Target: 0.8', fontsize=9, color='green', ha='center')\n",
    "ax2.grid(axis='x', alpha=0.2)\n",
    "\n",
    "# 3. F1 Score vs Threshold (Bottom left)\n",
    "ax3 = fig.add_subplot(gs[1, 0])\n",
    "\n",
    "has_threshold_data = False\n",
    "for class_name, pr_stats in pr_analysis.items():\n",
    "    if 'thresholds' in pr_stats and 'f1_scores' in pr_stats:\n",
    "        has_threshold_data = True\n",
    "        thresholds = np.array(pr_stats['thresholds'])\n",
    "        f1_scores = np.array(pr_stats['f1_scores'])\n",
    "        \n",
    "        # Smooth the curve if too many points\n",
    "        if len(thresholds) > 100:\n",
    "            # Sample points for smoother visualization\n",
    "            indices = np.linspace(0, len(thresholds)-1, 100, dtype=int)\n",
    "            thresholds = thresholds[indices]\n",
    "            f1_scores = f1_scores[indices]\n",
    "        \n",
    "        ax3.plot(thresholds, f1_scores,\n",
    "                color=colors.get(class_name, '#1976D2'),\n",
    "                linewidth=2, label=class_name, alpha=0.9)\n",
    "        \n",
    "        # Mark optimal\n",
    "        optimal_idx = np.argmax(pr_stats['f1_scores'])\n",
    "        if optimal_idx < len(pr_stats['thresholds']):\n",
    "            ax3.plot(pr_stats['thresholds'][optimal_idx],\n",
    "                    pr_stats['f1_scores'][optimal_idx],\n",
    "                    marker=markers.get(class_name, 'o'),\n",
    "                    markersize=10,\n",
    "                    color=colors.get(class_name, '#1976D2'),\n",
    "                    markeredgecolor='white',\n",
    "                    markeredgewidth=2)\n",
    "\n",
    "ax3.set_xlabel('Confidence Threshold', fontsize=11, fontweight='bold')\n",
    "ax3.set_ylabel('F1 Score', fontsize=11, fontweight='bold')\n",
    "ax3.set_title('F1 Score vs Threshold', fontsize=12, fontweight='bold')\n",
    "ax3.set_xlim([0, 1])\n",
    "ax3.set_ylim([0, 1.05])\n",
    "ax3.axvline(x=0.5, color='gray', linestyle=':', alpha=0.3, linewidth=1)\n",
    "ax3.axhline(y=0.8, color='green', linestyle=':', alpha=0.3, linewidth=1)\n",
    "ax3.grid(True, alpha=0.2)\n",
    "ax3.legend(loc='best', fontsize=10)\n",
    "\n",
    "# 4. Optimal Operating Points (Bottom middle)\n",
    "ax4 = fig.add_subplot(gs[1, 1])\n",
    "x = np.arange(len(classes))\n",
    "width = 0.35\n",
    "\n",
    "thresholds = [pr_analysis[c]['optimal_threshold'] for c in classes]\n",
    "f1_scores = [pr_analysis[c]['max_f1'] for c in classes]\n",
    "\n",
    "bars1 = ax4.bar(x - width/2, thresholds, width, label='Optimal Threshold',\n",
    "                color='#1976D2', edgecolor='white', linewidth=2, alpha=0.85)\n",
    "bars2 = ax4.bar(x + width/2, f1_scores, width, label='Max F1 Score',\n",
    "                color='#FF6F00', edgecolor='white', linewidth=2, alpha=0.85)\n",
    "\n",
    "# Add value labels\n",
    "for bar, val in zip(bars1, thresholds):\n",
    "    ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,\n",
    "             f'{val:.2f}', ha='center', fontsize=9, fontweight='bold')\n",
    "for bar, val in zip(bars2, f1_scores):\n",
    "    ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,\n",
    "             f'{val:.2f}', ha='center', fontsize=9, fontweight='bold')\n",
    "\n",
    "ax4.set_ylabel('Score', fontsize=11, fontweight='bold')\n",
    "ax4.set_title('Optimal Operating Points', fontsize=12, fontweight='bold')\n",
    "ax4.set_xticks(x)\n",
    "ax4.set_xticklabels(classes)\n",
    "ax4.legend(loc='upper right', fontsize=10)\n",
    "ax4.set_ylim(0, 1.15)\n",
    "ax4.grid(axis='y', alpha=0.2)\n",
    "ax4.axhline(y=0.5, color='gray', linestyle=':', alpha=0.3, linewidth=1)\n",
    "\n",
    "# 5. Performance Summary (Bottom right)\n",
    "ax5 = fig.add_subplot(gs[1, 2])\n",
    "ax5.axis('off')\n",
    "\n",
    "# Create summary text\n",
    "summary_text = \"Performance Summary\\n\" + \"=\"*25 + \"\\n\\n\"\n",
    "\n",
    "for class_name, pr_stats in pr_analysis.items():\n",
    "    summary_text += f\"{class_name}:\\n\"\n",
    "    summary_text += f\"  • AP: {pr_stats['ap']:.3f}\\n\"\n",
    "    summary_text += f\"  • Best F1: {pr_stats['max_f1']:.3f}\\n\"\n",
    "    summary_text += f\"  • Optimal Threshold: {pr_stats['optimal_threshold']:.2f}\\n\"\n",
    "    summary_text += f\"  • P@Optimal: {pr_stats['precision_at_optimal']:.3f}\\n\"\n",
    "    summary_text += f\"  • R@Optimal: {pr_stats['recall_at_optimal']:.3f}\\n\\n\"\n",
    "\n",
    "# Calculate overall performance\n",
    "mean_ap = np.mean([pr_stats['ap'] for pr_stats in pr_analysis.values()])\n",
    "summary_text += f\"Mean AP: {mean_ap:.3f}\"\n",
    "\n",
    "ax5.text(0.1, 0.9, summary_text, transform=ax5.transAxes,\n",
    "         fontsize=10, fontfamily='monospace',\n",
    "         verticalalignment='top',\n",
    "         bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.1))\n",
    "\n",
    "# Overall title\n",
    "fig.suptitle('Precision-Recall Analysis Dashboard', fontsize=16, fontweight='bold', y=0.98)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.02, 1, 0.96])\n",
    "save_path = Path('../results') / experiment_name / 'pr_curves_comprehensive.png'\n",
    "plt.savefig(save_path, dpi=300, bbox_inches='tight', facecolor='white')\n",
    "plt.show()\n",
    "\n",
    "# Key insights\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"KEY INSIGHTS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Performance assessment\n",
    "mean_ap = np.mean([pr_stats['ap'] for pr_stats in pr_analysis.values()])\n",
    "print(f\"Overall Mean AP: {mean_ap:.3f}\")\n",
    "\n",
    "if mean_ap >= 0.8:\n",
    "    print(\"→ Excellent overall performance\")\n",
    "elif mean_ap >= 0.6:\n",
    "    print(\"→ Good overall performance\")\n",
    "else:\n",
    "    print(\"→ Performance needs improvement\")\n",
    "\n",
    "# Class balance\n",
    "ap_diff = abs(pr_analysis['Infected']['ap'] - pr_analysis['Uninfected']['ap'])\n",
    "if ap_diff > 0.1:\n",
    "    print(f\"\\n⚠️ Significant AP difference between classes ({ap_diff:.3f})\")\n",
    "    print(\"   Consider class balancing techniques\")\n",
    "\n",
    "# Threshold recommendations\n",
    "for class_name, pr_stats in pr_analysis.items():\n",
    "    optimal = pr_stats.get('optimal_threshold', 0.5)\n",
    "    if abs(optimal - 0.5) > 0.1:\n",
    "        print(f\"\\n{class_name}: Optimal threshold ({optimal:.3f}) differs from default (0.5)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6b115f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: TIDE Error Analysis - COMPLETE CORRECTED VERSION\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TIDE ERROR ANALYSIS - CORRECTED\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def compute_tide_errors_complete(evaluator, split='test', max_images=100):\n",
    "    \"\"\"Complete TIDE error analysis with CORRECTED per-class tracking\"\"\"\n",
    "    \n",
    "    img_dir = evaluator.dataset_path / split / \"images\"\n",
    "    lbl_dir = evaluator.dataset_path / split / \"labels\"\n",
    "    \n",
    "    # Initialize raw counters\n",
    "    errors_per_class_raw = {}\n",
    "    for class_name in evaluator.class_names.values():\n",
    "        errors_per_class_raw[class_name] = {\n",
    "            'classification': [],  # When THIS class GT is misclassified\n",
    "            'localization': [],    # When correctly classified but poorly localized\n",
    "            'duplicate': [],        # When THIS class has duplicate detections\n",
    "            'background': [],       # When THIS class is falsely detected (no GT)\n",
    "            'missed': []           # When THIS class GT is not detected\n",
    "        }\n",
    "    \n",
    "    errors_aggregate_raw = {\n",
    "        'classification': [],\n",
    "        'localization': [],\n",
    "        'duplicate': [],\n",
    "        'background': [],\n",
    "        'missed': []\n",
    "    }\n",
    "    \n",
    "    img_files = list(img_dir.glob('*.jpg'))[:max_images]\n",
    "    \n",
    "    for img_path in tqdm(img_files, desc=\"Computing TIDE errors\"):\n",
    "        label_path = lbl_dir / (img_path.stem + '.txt')\n",
    "        \n",
    "        # Load GT boxes\n",
    "        gt_boxes = []\n",
    "        if label_path.exists():\n",
    "            img = Image.open(img_path)\n",
    "            img_width, img_height = img.size\n",
    "            \n",
    "            with open(label_path, 'r') as f:\n",
    "                for line in f:\n",
    "                    parts = line.strip().split()\n",
    "                    if len(parts) >= 5:\n",
    "                        class_id = int(parts[0])\n",
    "                        x_center = float(parts[1]) * img_width\n",
    "                        y_center = float(parts[2]) * img_height\n",
    "                        width = float(parts[3]) * img_width\n",
    "                        height = float(parts[4]) * img_height\n",
    "                        \n",
    "                        x1 = x_center - width/2\n",
    "                        y1 = y_center - height/2\n",
    "                        x2 = x_center + width/2\n",
    "                        y2 = y_center + height/2\n",
    "                        \n",
    "                        gt_boxes.append({\n",
    "                            'class_id': class_id,\n",
    "                            'box': [x1, y1, x2, y2],\n",
    "                            'matched': False\n",
    "                        })\n",
    "        \n",
    "        # Get predictions\n",
    "        results = evaluator.model.predict(img_path, conf=0.5, iou=0.5, verbose=False)[0]\n",
    "        pred_boxes = []\n",
    "        \n",
    "        if results.boxes is not None:\n",
    "            for box in results.boxes:\n",
    "                pred_boxes.append({\n",
    "                    'class_id': int(box.cls.item()),\n",
    "                    'box': box.xyxy[0].tolist(),\n",
    "                    'matched': False\n",
    "                })\n",
    "        \n",
    "        # Match predictions to GT\n",
    "        for pred_idx, pred in enumerate(pred_boxes):\n",
    "            best_iou = 0\n",
    "            best_gt_idx = -1\n",
    "            \n",
    "            for gt_idx, gt in enumerate(gt_boxes):\n",
    "                if gt['matched']:\n",
    "                    continue\n",
    "                \n",
    "                iou = evaluator._compute_iou(pred['box'], gt['box'])\n",
    "                if iou > best_iou:\n",
    "                    best_iou = iou\n",
    "                    best_gt_idx = gt_idx\n",
    "            \n",
    "            pred_class_name = evaluator.class_names[pred['class_id']]\n",
    "            \n",
    "            if best_iou >= 0.5:  # Matched with GT\n",
    "                gt = gt_boxes[best_gt_idx]\n",
    "                gt_class_name = evaluator.class_names[gt['class_id']]\n",
    "                \n",
    "                if pred['class_id'] != gt['class_id']:\n",
    "                    # Classification error - count ONLY for the GT class (what was actually there)\n",
    "                    errors_per_class_raw[gt_class_name]['classification'].append(1)\n",
    "                    errors_aggregate_raw['classification'].append(1)\n",
    "                else:\n",
    "                    # Correct class but localization error\n",
    "                    loc_error = 1 - best_iou\n",
    "                    errors_per_class_raw[pred_class_name]['localization'].append(loc_error)\n",
    "                    errors_aggregate_raw['localization'].append(loc_error)\n",
    "                \n",
    "                gt['matched'] = True\n",
    "                pred['matched'] = True\n",
    "                \n",
    "            else:  # No matching GT - background/false positive error\n",
    "                errors_per_class_raw[pred_class_name]['background'].append(1)\n",
    "                errors_aggregate_raw['background'].append(1)\n",
    "        \n",
    "        # Check for duplicate predictions\n",
    "        for i in range(len(pred_boxes)):\n",
    "            if pred_boxes[i]['matched']:\n",
    "                continue\n",
    "            for j in range(i+1, len(pred_boxes)):\n",
    "                if pred_boxes[j]['matched']:\n",
    "                    continue\n",
    "                if pred_boxes[i]['class_id'] == pred_boxes[j]['class_id']:\n",
    "                    iou = evaluator._compute_iou(pred_boxes[i]['box'], pred_boxes[j]['box'])\n",
    "                    if iou > 0.5:\n",
    "                        class_name = evaluator.class_names[pred_boxes[i]['class_id']]\n",
    "                        errors_per_class_raw[class_name]['duplicate'].append(1)\n",
    "                        errors_aggregate_raw['duplicate'].append(1)\n",
    "                        pred_boxes[j]['matched'] = True  # Mark to avoid re-counting\n",
    "        \n",
    "        # Check for missed detections (unmatched GT)\n",
    "        for gt in gt_boxes:\n",
    "            if not gt['matched']:\n",
    "                gt_class_name = evaluator.class_names[gt['class_id']]\n",
    "                errors_per_class_raw[gt_class_name]['missed'].append(1)\n",
    "                errors_aggregate_raw['missed'].append(1)\n",
    "    \n",
    "    # Calculate statistics\n",
    "    n_images = len(img_files)\n",
    "    \n",
    "    final_errors_per_class = {}\n",
    "    for class_name in evaluator.class_names.values():\n",
    "        final_errors_per_class[class_name] = {}\n",
    "        for error_type in ['classification', 'localization', 'duplicate', 'background', 'missed']:\n",
    "            values = errors_per_class_raw[class_name][error_type]\n",
    "            if values:\n",
    "                final_errors_per_class[class_name][error_type] = {\n",
    "                    'mean': np.mean(values),\n",
    "                    'std': np.std(values),\n",
    "                    'count': len(values),\n",
    "                    'rate_per_image': len(values) / n_images\n",
    "                }\n",
    "            else:\n",
    "                final_errors_per_class[class_name][error_type] = {\n",
    "                    'mean': 0,\n",
    "                    'std': 0,\n",
    "                    'count': 0,\n",
    "                    'rate_per_image': 0\n",
    "                }\n",
    "    \n",
    "    final_errors_aggregate = {}\n",
    "    for error_type in ['classification', 'localization', 'duplicate', 'background', 'missed']:\n",
    "        values = errors_aggregate_raw[error_type]\n",
    "        if values:\n",
    "            final_errors_aggregate[error_type] = {\n",
    "                'mean': np.mean(values),\n",
    "                'std': np.std(values),\n",
    "                'count': len(values),\n",
    "                'rate_per_image': len(values) / n_images\n",
    "            }\n",
    "        else:\n",
    "            final_errors_aggregate[error_type] = {\n",
    "                'mean': 0,\n",
    "                'std': 0,\n",
    "                'count': 0,\n",
    "                'rate_per_image': 0\n",
    "            }\n",
    "    \n",
    "    return final_errors_aggregate, final_errors_per_class, n_images\n",
    "\n",
    "# Run the CORRECTED TIDE analysis\n",
    "errors_aggregate_fixed, errors_per_class_fixed, n_images_analyzed = compute_tide_errors_complete(\n",
    "    evaluator, split='test', max_images=100\n",
    ")\n",
    "\n",
    "# Store in test_results\n",
    "test_results['errors'] = errors_aggregate_fixed\n",
    "test_results['errors_per_class'] = errors_per_class_fixed\n",
    "\n",
    "# Display results in table format\n",
    "error_headers = ['Class', 'Classification', 'Localization', 'Background (FP)', 'Missed (FN)', 'Duplicate']\n",
    "error_data = []\n",
    "\n",
    "for class_name in config.get_class_names().values():\n",
    "    class_errors = errors_per_class_fixed.get(class_name, {})\n",
    "    error_data.append([\n",
    "        class_name,\n",
    "        f\"{class_errors.get('classification', {}).get('rate_per_image', 0):.2f} ({class_errors.get('classification', {}).get('count', 0)})\",\n",
    "        f\"{class_errors.get('localization', {}).get('mean', 0):.3f} ({class_errors.get('localization', {}).get('count', 0)})\",\n",
    "        f\"{class_errors.get('background', {}).get('rate_per_image', 0):.2f} ({class_errors.get('background', {}).get('count', 0)})\",\n",
    "        f\"{class_errors.get('missed', {}).get('rate_per_image', 0):.2f} ({class_errors.get('missed', {}).get('count', 0)})\",\n",
    "        f\"{class_errors.get('duplicate', {}).get('rate_per_image', 0):.2f} ({class_errors.get('duplicate', {}).get('count', 0)})\"\n",
    "    ])\n",
    "\n",
    "# Add separator and aggregate\n",
    "error_data.append(['─'*15, '─'*20, '─'*20, '─'*20, '─'*20, '─'*20])\n",
    "error_data.append([\n",
    "    'AGGREGATE',\n",
    "    f\"{errors_aggregate_fixed.get('classification', {}).get('rate_per_image', 0):.2f} ({errors_aggregate_fixed.get('classification', {}).get('count', 0)})\",\n",
    "    f\"{errors_aggregate_fixed.get('localization', {}).get('mean', 0):.3f} ({errors_aggregate_fixed.get('localization', {}).get('count', 0)})\",\n",
    "    f\"{errors_aggregate_fixed.get('background', {}).get('rate_per_image', 0):.2f} ({errors_aggregate_fixed.get('background', {}).get('count', 0)})\",\n",
    "    f\"{errors_aggregate_fixed.get('missed', {}).get('rate_per_image', 0):.2f} ({errors_aggregate_fixed.get('missed', {}).get('count', 0)})\",\n",
    "    f\"{errors_aggregate_fixed.get('duplicate', {}).get('rate_per_image', 0):.2f} ({errors_aggregate_fixed.get('duplicate', {}).get('count', 0)})\"\n",
    "])\n",
    "\n",
    "print(\"\\n\" + tabulate(error_data,\n",
    "                      headers=error_headers,\n",
    "                      tablefmt='fancy_grid',\n",
    "                      numalign='right'))\n",
    "print(f\"\\n*Values show: rate_per_image (total_count) | Analyzed {n_images_analyzed} images\")\n",
    "print(\"\\nError Type Definitions:\")\n",
    "print(\"- Classification: GT object detected but wrong class predicted\")\n",
    "print(\"- Localization: Correct class but poor bounding box (IoU < 0.5)\")\n",
    "print(\"- Background (FP): Detection where no GT exists\")\n",
    "print(\"- Missed (FN): GT object not detected\")\n",
    "print(\"- Duplicate: Multiple detections for same object\")\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle('TIDE Error Analysis', fontsize=14, fontweight='bold')\n",
    "\n",
    "# 1. Error distribution by type and class\n",
    "ax = axes[0, 0]\n",
    "error_types = ['Classification', 'Background', 'Missed']\n",
    "classes = list(config.get_class_names().values())\n",
    "\n",
    "uninfected_counts = [\n",
    "    errors_per_class_fixed['Uninfected']['classification']['count'],\n",
    "    errors_per_class_fixed['Uninfected']['background']['count'],\n",
    "    errors_per_class_fixed['Uninfected']['missed']['count']\n",
    "]\n",
    "infected_counts = [\n",
    "    errors_per_class_fixed['Infected']['classification']['count'],\n",
    "    errors_per_class_fixed['Infected']['background']['count'],\n",
    "    errors_per_class_fixed['Infected']['missed']['count']\n",
    "]\n",
    "\n",
    "x = np.arange(len(error_types))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax.bar(x - width/2, uninfected_counts, width, label='Uninfected', \n",
    "               color='green', alpha=0.7, edgecolor='black')\n",
    "bars2 = ax.bar(x + width/2, infected_counts, width, label='Infected', \n",
    "               color='red', alpha=0.7, edgecolor='black')\n",
    "\n",
    "ax.set_xlabel('Error Type', fontsize=11)\n",
    "ax.set_ylabel('Count', fontsize=11)\n",
    "ax.set_title('Error Distribution by Class', fontsize=12)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(error_types)\n",
    "ax.legend()\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        if height > 0:\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height + max(uninfected_counts + infected_counts)*0.02,\n",
    "                   f'{int(height)}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# 2. Error rate per image\n",
    "ax = axes[0, 1]\n",
    "error_rates = {\n",
    "    'Uninfected\\nFP': errors_per_class_fixed['Uninfected']['background']['rate_per_image'],\n",
    "    'Uninfected\\nMissed': errors_per_class_fixed['Uninfected']['missed']['rate_per_image'],\n",
    "    'Infected\\nFP': errors_per_class_fixed['Infected']['background']['rate_per_image'],\n",
    "    'Infected\\nMissed': errors_per_class_fixed['Infected']['missed']['rate_per_image']\n",
    "}\n",
    "\n",
    "bars = ax.bar(error_rates.keys(), error_rates.values(), \n",
    "              color=['lightgreen', 'darkgreen', 'lightcoral', 'darkred'],\n",
    "              edgecolor='black', linewidth=1, alpha=0.8)\n",
    "\n",
    "ax.set_ylabel('Errors per Image', fontsize=11)\n",
    "ax.set_title('Average Error Rate per Image', fontsize=12)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height + 0.05,\n",
    "           f'{height:.2f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# 3. Localization quality\n",
    "ax = axes[1, 0]\n",
    "loc_data = []\n",
    "loc_labels = []\n",
    "loc_counts = []\n",
    "for class_name in classes:\n",
    "    if errors_per_class_fixed[class_name]['localization']['count'] > 0:\n",
    "        loc_data.append(errors_per_class_fixed[class_name]['localization']['mean'])\n",
    "        loc_labels.append(class_name)\n",
    "        loc_counts.append(errors_per_class_fixed[class_name]['localization']['count'])\n",
    "\n",
    "if loc_data:\n",
    "    bars = ax.bar(loc_labels, loc_data, color=['green', 'red'], alpha=0.7, edgecolor='black')\n",
    "    ax.set_ylabel('Mean Localization Error (1-IoU)', fontsize=11)\n",
    "    ax.set_title('Localization Quality When Detected', fontsize=12)\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    ax.set_ylim(0, max(0.5, max(loc_data) * 1.2))\n",
    "    \n",
    "    for bar, count in zip(bars, loc_counts):\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height + 0.005,\n",
    "               f'{height:.3f}\\n(n={count})', ha='center', va='bottom', fontsize=9)\n",
    "else:\n",
    "    ax.text(0.5, 0.5, 'No localization data', ha='center', va='center')\n",
    "\n",
    "# 4. Clinical impact for infected cells\n",
    "ax = axes[1, 1]\n",
    "infected_detected = errors_per_class_fixed['Infected']['localization']['count']\n",
    "infected_missed = errors_per_class_fixed['Infected']['missed']['count']\n",
    "infected_misclassified = errors_per_class_fixed['Infected']['classification']['count']\n",
    "infected_fp = errors_per_class_fixed['Infected']['background']['count']\n",
    "\n",
    "# Total infected GT = correctly detected + missed + misclassified\n",
    "infected_total_gt = infected_detected + infected_missed + infected_misclassified\n",
    "\n",
    "impact_data = {\n",
    "    'Correctly\\nDetected': infected_detected,\n",
    "    'Missed': infected_missed,\n",
    "    'Misclassified': infected_misclassified,\n",
    "    'False\\nPositives': infected_fp\n",
    "}\n",
    "\n",
    "colors_impact = ['green', 'red', 'orange', 'purple']\n",
    "bars = ax.bar(impact_data.keys(), impact_data.values(), \n",
    "              color=colors_impact, alpha=0.7, edgecolor='black')\n",
    "\n",
    "ax.set_ylabel('Count', fontsize=11)\n",
    "ax.set_title('Infected Cell Detection Breakdown', fontsize=12)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height + 2,\n",
    "           f'{int(height)}', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "if infected_total_gt > 0:\n",
    "    detection_rate = (infected_detected / infected_total_gt) * 100\n",
    "    ax.text(0.5, 0.95, f'Detection Rate: {detection_rate:.1f}%',\n",
    "           transform=ax.transAxes, ha='center', fontsize=11,\n",
    "           bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "save_path = Path('../results') / experiment_name / 'tide_error_analysis.png'\n",
    "plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFigure saved to: {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca263d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 17: Enhanced Ground Truth vs Predictions with Full Export\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"GROUND TRUTH VS PREDICTIONS - VISUALIZATION & EXPORT\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def save_all_predictions(evaluator, split='test', output_folder='predictions_output'):\n",
    "    \"\"\"Save predictions for ALL images in the dataset\"\"\"\n",
    "    \n",
    "    img_dir = evaluator.dataset_path / split / \"images\"\n",
    "    lbl_dir = evaluator.dataset_path / split / \"labels\"\n",
    "    \n",
    "    # Create output directory\n",
    "    output_path = Path('../results') / experiment_name / output_folder / split\n",
    "    output_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Get all images\n",
    "    img_files = list(img_dir.glob('*.jpg')) + list(img_dir.glob('*.png'))\n",
    "    \n",
    "    print(f\"Saving predictions for {len(img_files)} images to: {output_path}\")\n",
    "    \n",
    "    # Define colors\n",
    "    colors = {'Uninfected': 'green', 'Infected': 'red'}\n",
    "    \n",
    "    for img_path in tqdm(img_files, desc=\"Saving predictions\"):\n",
    "        label_path = lbl_dir / (img_path.stem + '.txt')\n",
    "        \n",
    "        # Create figure for this image\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "        \n",
    "        # Load image\n",
    "        img = Image.open(img_path)\n",
    "        img_array = np.array(img)\n",
    "        img_width, img_height = img.size\n",
    "        \n",
    "        # Count GT boxes\n",
    "        gt_counts = {'Uninfected': 0, 'Infected': 0}\n",
    "        \n",
    "        # LEFT: Ground Truth\n",
    "        ax_gt = axes[0]\n",
    "        ax_gt.imshow(img_array)\n",
    "        ax_gt.set_title('Ground Truth', fontsize=14, fontweight='bold', pad=10)\n",
    "        ax_gt.axis('off')\n",
    "        \n",
    "        # Load and draw GT boxes\n",
    "        if label_path.exists():\n",
    "            with open(label_path, 'r') as f:\n",
    "                for line in f:\n",
    "                    parts = line.strip().split()\n",
    "                    if len(parts) >= 5:\n",
    "                        class_id = int(parts[0])\n",
    "                        class_name = evaluator.class_names[class_id]\n",
    "                        gt_counts[class_name] += 1\n",
    "                        \n",
    "                        x_center = float(parts[1]) * img_width\n",
    "                        y_center = float(parts[2]) * img_height\n",
    "                        width = float(parts[3]) * img_width\n",
    "                        height = float(parts[4]) * img_height\n",
    "                        \n",
    "                        x1 = x_center - width/2\n",
    "                        y1 = y_center - height/2\n",
    "                        \n",
    "                        # Draw box\n",
    "                        rect = patches.Rectangle(\n",
    "                            (x1, y1), width, height,\n",
    "                            linewidth=2,\n",
    "                            edgecolor=colors[class_name],\n",
    "                            facecolor='none'\n",
    "                        )\n",
    "                        ax_gt.add_patch(rect)\n",
    "        \n",
    "        # Add GT statistics\n",
    "        gt_total = sum(gt_counts.values())\n",
    "        gt_stats = f\"Total: {gt_total} | Infected: {gt_counts['Infected']} | Uninfected: {gt_counts['Uninfected']}\"\n",
    "        ax_gt.text(0.5, -0.08, gt_stats, transform=ax_gt.transAxes,\n",
    "                  ha='center', fontsize=11, bbox=dict(boxstyle='round', \n",
    "                  facecolor='lightgray', alpha=0.8))\n",
    "        \n",
    "        # RIGHT: Predictions\n",
    "        ax_pred = axes[1]\n",
    "        ax_pred.imshow(img_array)\n",
    "        ax_pred.set_title('Model Predictions', fontsize=14, fontweight='bold', pad=10)\n",
    "        ax_pred.axis('off')\n",
    "        \n",
    "        # Get predictions\n",
    "        pred_counts = {'Uninfected': 0, 'Infected': 0}\n",
    "        results = evaluator.model.predict(img_path, conf=0.5, iou=0.5, verbose=False)[0]\n",
    "        \n",
    "        if results.boxes is not None:\n",
    "            for box in results.boxes:\n",
    "                x1, y1, x2, y2 = box.xyxy[0].tolist()\n",
    "                class_id = int(box.cls.item())\n",
    "                class_name = evaluator.class_names[class_id]\n",
    "                conf = box.conf.item()\n",
    "                pred_counts[class_name] += 1\n",
    "                \n",
    "                # Draw box\n",
    "                rect = patches.Rectangle(\n",
    "                    (x1, y1), x2-x1, y2-y1,\n",
    "                    linewidth=2,\n",
    "                    edgecolor=colors[class_name],\n",
    "                    facecolor='none',\n",
    "                    linestyle='--'\n",
    "                )\n",
    "                ax_pred.add_patch(rect)\n",
    "                \n",
    "                # Add confidence score\n",
    "                ax_pred.text(x1 + 2, y1 + 10, f'{conf:.2f}',\n",
    "                            color=colors[class_name],\n",
    "                            fontsize=8,\n",
    "                            fontweight='bold',\n",
    "                            bbox=dict(facecolor='white', alpha=0.7, pad=1))\n",
    "        \n",
    "        # Add prediction statistics\n",
    "        pred_total = sum(pred_counts.values())\n",
    "        pred_stats = f\"Total: {pred_total} | Infected: {pred_counts['Infected']} | Uninfected: {pred_counts['Uninfected']}\"\n",
    "        ax_pred.text(0.5, -0.08, pred_stats, transform=ax_pred.transAxes,\n",
    "                    ha='center', fontsize=11, bbox=dict(boxstyle='round', \n",
    "                    facecolor='lightgray', alpha=0.8))\n",
    "        \n",
    "        # Add image name at the top with proper spacing\n",
    "        fig.suptitle(f\"Image: {img_path.stem}\", fontsize=13, fontweight='bold', y=0.98)\n",
    "        \n",
    "        # Add legend at bottom\n",
    "        legend_elements = [\n",
    "            patches.Rectangle((0, 0), 1, 1, facecolor='none', edgecolor='green', \n",
    "                             linewidth=2, label='Uninfected'),\n",
    "            patches.Rectangle((0, 0), 1, 1, facecolor='none', edgecolor='red', \n",
    "                             linewidth=2, label='Infected'),\n",
    "            patches.Rectangle((0, 0), 1, 1, facecolor='none', edgecolor='gray', \n",
    "                             linewidth=2, linestyle='-', label='Ground Truth'),\n",
    "            patches.Rectangle((0, 0), 1, 1, facecolor='none', edgecolor='gray', \n",
    "                             linewidth=2, linestyle='--', label='Prediction')\n",
    "        ]\n",
    "        \n",
    "        fig.legend(handles=legend_elements, \n",
    "                  loc='lower center',\n",
    "                  ncol=4,\n",
    "                  fontsize=10,\n",
    "                  frameon=True,\n",
    "                  bbox_to_anchor=(0.5, -0.02))\n",
    "        \n",
    "        plt.tight_layout(rect=[0, 0.03, 1, 0.97])\n",
    "        \n",
    "        # Save figure\n",
    "        save_file = output_path / f\"{img_path.stem}_predictions.png\"\n",
    "        plt.savefig(save_file, dpi=150, bbox_inches='tight')\n",
    "        plt.close(fig)  # Close to free memory\n",
    "    \n",
    "    print(f\"✓ All {len(img_files)} prediction images saved to: {output_path}\")\n",
    "    return output_path\n",
    "\n",
    "def visualize_sample_predictions(evaluator, split='test', num_samples=6):\n",
    "    \"\"\"Visualize a few sample predictions in the notebook\"\"\"\n",
    "    \n",
    "    img_dir = evaluator.dataset_path / split / \"images\"\n",
    "    lbl_dir = evaluator.dataset_path / split / \"labels\"\n",
    "    \n",
    "    # Get sample images\n",
    "    img_files = list(img_dir.glob('*.jpg'))[:num_samples]\n",
    "    \n",
    "    # Calculate grid dimensions\n",
    "    n_rows = (num_samples + 1) // 2  # 2 columns per row\n",
    "    \n",
    "    # Create figure with proper spacing\n",
    "    fig = plt.figure(figsize=(16, 8*n_rows))\n",
    "    fig.suptitle('Sample Predictions: Ground Truth vs Model Output', \n",
    "                fontsize=16, fontweight='bold', y=1.0)\n",
    "    \n",
    "    # Create grid with spacing\n",
    "    gs = fig.add_gridspec(n_rows, 2, hspace=0.3, wspace=0.15)\n",
    "    \n",
    "    # Define colors\n",
    "    colors = {'Uninfected': 'green', 'Infected': 'red'}\n",
    "    \n",
    "    for idx, img_path in enumerate(img_files):\n",
    "        row = idx // 2\n",
    "        col = idx % 2\n",
    "        \n",
    "        # Create sub-gridspec for each image pair (GT and Pred)\n",
    "        inner_gs = gs[row, col].subgridspec(1, 2, wspace=0.05)\n",
    "        ax_gt = fig.add_subplot(inner_gs[0, 0])\n",
    "        ax_pred = fig.add_subplot(inner_gs[0, 1])\n",
    "        \n",
    "        label_path = lbl_dir / (img_path.stem + '.txt')\n",
    "        \n",
    "        # Load image\n",
    "        img = Image.open(img_path)\n",
    "        img_array = np.array(img)\n",
    "        img_width, img_height = img.size\n",
    "        \n",
    "        # Count GT boxes\n",
    "        gt_counts = {'Uninfected': 0, 'Infected': 0}\n",
    "        \n",
    "        # LEFT: Ground Truth\n",
    "        ax_gt.imshow(img_array)\n",
    "        ax_gt.set_title('Ground Truth', fontsize=11, loc='left')\n",
    "        ax_gt.axis('off')\n",
    "        \n",
    "        # Load and draw GT boxes\n",
    "        if label_path.exists():\n",
    "            with open(label_path, 'r') as f:\n",
    "                for line in f:\n",
    "                    parts = line.strip().split()\n",
    "                    if len(parts) >= 5:\n",
    "                        class_id = int(parts[0])\n",
    "                        class_name = evaluator.class_names[class_id]\n",
    "                        gt_counts[class_name] += 1\n",
    "                        \n",
    "                        x_center = float(parts[1]) * img_width\n",
    "                        y_center = float(parts[2]) * img_height\n",
    "                        width = float(parts[3]) * img_width\n",
    "                        height = float(parts[4]) * img_height\n",
    "                        \n",
    "                        x1 = x_center - width/2\n",
    "                        y1 = y_center - height/2\n",
    "                        \n",
    "                        rect = patches.Rectangle(\n",
    "                            (x1, y1), width, height,\n",
    "                            linewidth=1.5,\n",
    "                            edgecolor=colors[class_name],\n",
    "                            facecolor='none'\n",
    "                        )\n",
    "                        ax_gt.add_patch(rect)\n",
    "        \n",
    "        # RIGHT: Predictions\n",
    "        ax_pred.imshow(img_array)\n",
    "        ax_pred.set_title('Predictions', fontsize=11, loc='left')\n",
    "        ax_pred.axis('off')\n",
    "        \n",
    "        # Get predictions\n",
    "        pred_counts = {'Uninfected': 0, 'Infected': 0}\n",
    "        results = evaluator.model.predict(img_path, conf=0.5, iou=0.5, verbose=False)[0]\n",
    "        \n",
    "        if results.boxes is not None:\n",
    "            for box in results.boxes:\n",
    "                x1, y1, x2, y2 = box.xyxy[0].tolist()\n",
    "                class_id = int(box.cls.item())\n",
    "                class_name = evaluator.class_names[class_id]\n",
    "                conf = box.conf.item()\n",
    "                pred_counts[class_name] += 1\n",
    "                \n",
    "                rect = patches.Rectangle(\n",
    "                    (x1, y1), x2-x1, y2-y1,\n",
    "                    linewidth=1.5,\n",
    "                    edgecolor=colors[class_name],\n",
    "                    facecolor='none',\n",
    "                    linestyle='--'\n",
    "                )\n",
    "                ax_pred.add_patch(rect)\n",
    "                \n",
    "                # Add small confidence text\n",
    "                ax_pred.text(x1 + 1, y1 + 8, f'{conf:.2f}',\n",
    "                            color=colors[class_name],\n",
    "                            fontsize=7,\n",
    "                            fontweight='bold',\n",
    "                            bbox=dict(facecolor='white', alpha=0.6, pad=0.5))\n",
    "        \n",
    "        # Add image name and stats above the pair\n",
    "        image_title = f\"{img_path.stem[:20]}...\"\n",
    "        stats_text = f\"GT: I={gt_counts['Infected']}, U={gt_counts['Uninfected']} | \" \\\n",
    "                    f\"Pred: I={pred_counts['Infected']}, U={pred_counts['Uninfected']}\"\n",
    "        \n",
    "        # Position text above the image pair\n",
    "        fig.text(0.25 + col*0.5, 0.98 - (row/n_rows)*0.95, image_title,\n",
    "                ha='center', fontsize=10, fontweight='bold')\n",
    "        fig.text(0.25 + col*0.5, 0.96 - (row/n_rows)*0.95, stats_text,\n",
    "                ha='center', fontsize=9, style='italic')\n",
    "    \n",
    "    # Add legend at bottom\n",
    "    legend_elements = [\n",
    "        patches.Rectangle((0, 0), 1, 1, facecolor='none', edgecolor='green', \n",
    "                         linewidth=2, label='Uninfected'),\n",
    "        patches.Rectangle((0, 0), 1, 1, facecolor='none', edgecolor='red', \n",
    "                         linewidth=2, label='Infected'),\n",
    "        patches.Rectangle((0, 0), 1, 1, facecolor='none', edgecolor='gray', \n",
    "                         linewidth=2, linestyle='-', label='Ground Truth'),\n",
    "        patches.Rectangle((0, 0), 1, 1, facecolor='none', edgecolor='gray', \n",
    "                         linewidth=2, linestyle='--', label='Prediction')\n",
    "    ]\n",
    "    \n",
    "    fig.legend(handles=legend_elements, \n",
    "              loc='lower center',\n",
    "              ncol=4,\n",
    "              fontsize=11,\n",
    "              frameon=True,\n",
    "              fancybox=True,\n",
    "              shadow=True,\n",
    "              bbox_to_anchor=(0.5, -0.02/n_rows))\n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0.02, 1, 0.96])\n",
    "    return fig\n",
    "\n",
    "# Save ALL predictions to disk\n",
    "print(\"Saving all predictions...\")\n",
    "output_folder = save_all_predictions(evaluator, split='test')\n",
    "\n",
    "# Visualize sample predictions in notebook\n",
    "print(\"\\nVisualizing sample predictions...\")\n",
    "fig = visualize_sample_predictions(evaluator, split='test', num_samples=6)\n",
    "save_path = Path('../results') / experiment_name / 'sample_predictions_grid.png'\n",
    "plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n✓ Sample visualization saved to: {save_path}\")\n",
    "print(f\"✓ All predictions saved to: {output_folder}\")\n",
    "print(f\"\\nYou can find individual prediction images for all {len(list((yolo_path / 'test' / 'images').glob('*.jpg')))} test images in the output folder.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858241c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 18: Final Summary Report\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"COMPLETE EVALUATION SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create comprehensive comparison table\n",
    "summary_data = []\n",
    "summary_data.append(['Metric', 'Validation', 'Test', 'Δ (Test-Val)'])\n",
    "summary_data.append(['─'*20, '─'*12, '─'*12, '─'*12])\n",
    "\n",
    "# Global metrics\n",
    "val_map50 = val_results['global']['mAP50']\n",
    "test_map50 = test_results['global']['mAP50']\n",
    "summary_data.append(['mAP@0.5', \n",
    "                    f\"{val_map50:.4f}\",\n",
    "                    f\"{test_map50:.4f}\",\n",
    "                    f\"{test_map50-val_map50:+.4f}\"])\n",
    "\n",
    "val_map = val_results['global']['mAP50-95']\n",
    "test_map = test_results['global']['mAP50-95']\n",
    "summary_data.append(['mAP@[0.5:0.95]',\n",
    "                    f\"{val_map:.4f}\",\n",
    "                    f\"{test_map:.4f}\",\n",
    "                    f\"{test_map-val_map:+.4f}\"])\n",
    "\n",
    "# Per-class F1\n",
    "for class_name in config.get_class_names().values():\n",
    "    if class_name in val_results['per_class'] and class_name in test_results['per_class']:\n",
    "        val_f1 = val_results['per_class'][class_name]['f1_score']\n",
    "        test_f1 = test_results['per_class'][class_name]['f1_score']\n",
    "        summary_data.append([f\"{class_name} F1\",\n",
    "                           f\"{val_f1:.4f}\",\n",
    "                           f\"{test_f1:.4f}\",\n",
    "                           f\"{test_f1-val_f1:+.4f}\"])\n",
    "\n",
    "print(\"\\n\" + tabulate(summary_data, \n",
    "                      headers='firstrow',\n",
    "                      tablefmt='fancy_grid',\n",
    "                      numalign='right'))\n",
    "\n",
    "# Save complete results\n",
    "complete_results = {\n",
    "    'experiment': experiment_name,\n",
    "    'config': vars(config),\n",
    "    'validation_results': val_results,\n",
    "    'test_results': test_results,\n",
    "    'timestamp': str(datetime.now())\n",
    "}\n",
    "\n",
    "results_path = Path('../results') / experiment_name / 'complete_evaluation.json'\n",
    "with open(results_path, 'w') as f:\n",
    "    json.dump(complete_results, f, indent=2, default=str)\n",
    "\n",
    "print(f\"\\n✓ Complete evaluation results saved to: {results_path}\")\n",
    "print(f\"✓ All visualizations saved to: {Path('../results') / experiment_name}\")\n",
    "print(\"\\nEvaluation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57973c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 18: Enhanced Decision Analysis - Aligned with Cell 17 Conventions\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import json\n",
    "import pandas as pd\n",
    "from matplotlib.patches import Rectangle\n",
    "from datetime import datetime\n",
    "from scipy.ndimage import gaussian_filter, zoom\n",
    "\n",
    "# Clear confidence thresholds - NO AMBIGUITY\n",
    "CONF_HIGH = 0.50  # >= 0.50 is confident (matches Cell 17)\n",
    "CONF_LOW = 0.30   # >= 0.30 but < 0.50 is uncertain\n",
    "\n",
    "def calculate_iou(box1, box2):\n",
    "    \"\"\"Calculate IoU between two boxes\"\"\"\n",
    "    x1_min, y1_min, w1, h1 = box1\n",
    "    x2_min, y2_min, w2, h2 = box2\n",
    "    \n",
    "    x1_max = x1_min + w1\n",
    "    y1_max = y1_min + h1\n",
    "    x2_max = x2_min + w2\n",
    "    y2_max = y2_min + h2\n",
    "    \n",
    "    intersect_xmin = max(x1_min, x2_min)\n",
    "    intersect_ymin = max(y1_min, y2_min)\n",
    "    intersect_xmax = min(x1_max, x2_max)\n",
    "    intersect_ymax = min(y1_max, y2_max)\n",
    "    \n",
    "    if intersect_xmax < intersect_xmin or intersect_ymax < intersect_ymin:\n",
    "        return 0.0\n",
    "    \n",
    "    intersect_area = (intersect_xmax - intersect_xmin) * (intersect_ymax - intersect_ymin)\n",
    "    box1_area = w1 * h1\n",
    "    box2_area = w2 * h2\n",
    "    union_area = box1_area + box2_area - intersect_area\n",
    "    \n",
    "    return intersect_area / union_area if union_area > 0 else 0\n",
    "\n",
    "def calculate_proper_metrics(gt_boxes, pred_boxes, iou_threshold=0.5):\n",
    "    \"\"\"Calculate TP/FP/FN using IoU matching\"\"\"\n",
    "    tp, fp, fn = 0, 0, 0\n",
    "    matched_gt = set()\n",
    "    \n",
    "    for pred in pred_boxes:\n",
    "        best_iou = 0\n",
    "        best_gt_idx = -1\n",
    "        \n",
    "        for gt_idx, gt in enumerate(gt_boxes):\n",
    "            if gt_idx not in matched_gt:\n",
    "                iou = calculate_iou(gt, pred)\n",
    "                if iou > best_iou:\n",
    "                    best_iou = iou\n",
    "                    best_gt_idx = gt_idx\n",
    "        \n",
    "        if best_iou >= iou_threshold:\n",
    "            tp += 1\n",
    "            matched_gt.add(best_gt_idx)\n",
    "        else:\n",
    "            fp += 1\n",
    "    \n",
    "    fn = len(gt_boxes) - len(matched_gt)\n",
    "    return tp, fp, fn\n",
    "\n",
    "def analyze_model_decisions_enhanced(model, test_images, model_name, save_dir, num_samples_to_display=6):\n",
    "    \"\"\"\n",
    "    Analyze ALL test images with conventions matching Cell 17:\n",
    "    - Ground Truth: Solid lines\n",
    "    - Predictions: Indicated as predictions (dashed where possible)\n",
    "    - Confidence threshold: 0.5 (matching Cell 17)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create single output directory\n",
    "    decision_dir = save_dir / model_name / 'decision_analysis'\n",
    "    decision_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # For CSV output - ALL images (SINGLE SOURCE OF TRUTH)\n",
    "    csv_data = []\n",
    "    \n",
    "    print(f\"Analyzing ALL {len(test_images)} test images...\")\n",
    "    print(f\"Will display {num_samples_to_display} in notebook, save all {len(test_images)} visualizations\")\n",
    "    print(f\"Confidence thresholds: Confident≥{CONF_HIGH}, Uncertain=[{CONF_LOW},{CONF_HIGH})\")\n",
    "    print(f\"Visualization convention: GT=Solid lines, Pred=Dashed/Marked\")\n",
    "    \n",
    "    all_confidence_data = {'infected': [], 'uninfected': []}\n",
    "    \n",
    "    # ANALYZE AND VISUALIZE ALL IMAGES\n",
    "    for img_idx, img_path in enumerate(test_images):\n",
    "        img_id = f\"{img_path.stem[:12]}_{img_idx:03d}\"\n",
    "        \n",
    "        img = Image.open(img_path)\n",
    "        img_array = np.array(img)\n",
    "        height, width = img_array.shape[:2]\n",
    "        \n",
    "        # Get ground truth\n",
    "        label_path = img_path.parent.parent / 'labels' / f\"{img_path.stem}.txt\"\n",
    "        gt_infected = 0\n",
    "        gt_uninfected = 0\n",
    "        gt_infected_boxes = []\n",
    "        gt_uninfected_boxes = []\n",
    "        \n",
    "        if label_path.exists():\n",
    "            with open(label_path) as f:\n",
    "                for line in f:\n",
    "                    parts = line.strip().split()\n",
    "                    if len(parts) >= 5:\n",
    "                        cls_id = int(parts[0])\n",
    "                        cx, cy, w, h = map(float, parts[1:5])\n",
    "                        x1 = (cx - w/2) * width\n",
    "                        y1 = (cy - h/2) * height\n",
    "                        box_w = w * width\n",
    "                        box_h = h * height\n",
    "                        \n",
    "                        if cls_id == 0:\n",
    "                            gt_uninfected += 1\n",
    "                            gt_uninfected_boxes.append([x1, y1, box_w, box_h])\n",
    "                        else:\n",
    "                            gt_infected += 1\n",
    "                            gt_infected_boxes.append([x1, y1, box_w, box_h])\n",
    "        \n",
    "        # Get predictions (matching Cell 17's conf=0.5)\n",
    "        results_normal = model.predict(img_path, conf=CONF_HIGH, verbose=False)[0]\n",
    "        results_low = model.predict(img_path, conf=CONF_LOW, verbose=False)[0]\n",
    "        \n",
    "        pred_infected = 0\n",
    "        pred_uninfected = 0\n",
    "        pred_infected_conf = []\n",
    "        pred_uninfected_conf = []\n",
    "        pred_infected_boxes = []\n",
    "        pred_uninfected_boxes = []\n",
    "        \n",
    "        if results_normal.boxes is not None:\n",
    "            for box in results_normal.boxes:\n",
    "                cls = int(box.cls.item())\n",
    "                conf = box.conf.item()\n",
    "                x1, y1, x2, y2 = box.xyxy[0].tolist()\n",
    "                \n",
    "                if cls == 0:\n",
    "                    pred_uninfected += 1\n",
    "                    pred_uninfected_conf.append(conf)\n",
    "                    pred_uninfected_boxes.append([x1, y1, x2-x1, y2-y1])\n",
    "                    all_confidence_data['uninfected'].append(conf)\n",
    "                else:\n",
    "                    pred_infected += 1\n",
    "                    pred_infected_conf.append(conf)\n",
    "                    pred_infected_boxes.append([x1, y1, x2-x1, y2-y1])\n",
    "                    all_confidence_data['infected'].append(conf)\n",
    "        \n",
    "        # Count low confidence detections\n",
    "        low_conf_count = 0\n",
    "        low_conf_data = []\n",
    "        if results_low.boxes is not None:\n",
    "            for box in results_low.boxes:\n",
    "                conf = box.conf.item()\n",
    "                if CONF_LOW <= conf < CONF_HIGH:  # Explicit range\n",
    "                    low_conf_count += 1\n",
    "                    cls = int(box.cls.item())\n",
    "                    low_conf_data.append(f\"{cls}:{conf:.3f}\")\n",
    "        \n",
    "        # Calculate proper metrics using IoU\n",
    "        tp_infected, fp_infected, fn_infected = calculate_proper_metrics(\n",
    "            gt_infected_boxes, pred_infected_boxes, iou_threshold=0.5\n",
    "        )\n",
    "        tp_uninfected, fp_uninfected, fn_uninfected = calculate_proper_metrics(\n",
    "            gt_uninfected_boxes, pred_uninfected_boxes, iou_threshold=0.5\n",
    "        )\n",
    "        \n",
    "        # Calculate F1 scores safely\n",
    "        infected_precision = tp_infected / pred_infected if pred_infected > 0 else 0\n",
    "        infected_recall = tp_infected / gt_infected if gt_infected > 0 else 0\n",
    "        infected_f1 = 2 * (infected_precision * infected_recall) / (infected_precision + infected_recall) if (infected_precision + infected_recall) > 0 else 0\n",
    "        \n",
    "        uninfected_precision = tp_uninfected / pred_uninfected if pred_uninfected > 0 else 0\n",
    "        uninfected_recall = tp_uninfected / gt_uninfected if gt_uninfected > 0 else 0\n",
    "        uninfected_f1 = 2 * (uninfected_precision * uninfected_recall) / (uninfected_precision + uninfected_recall) if (uninfected_precision + uninfected_recall) > 0 else 0\n",
    "        \n",
    "        # Store EVERYTHING in ONE CSV row\n",
    "        csv_row = {\n",
    "            'Image_ID': img_id,\n",
    "            'Image_Path': img_path.name,\n",
    "            'Model_Strategy': model_name.split('_')[-1],\n",
    "            'Width': width,\n",
    "            'Height': height,\n",
    "            # Ground Truth\n",
    "            'GT_Infected': gt_infected,\n",
    "            'GT_Uninfected': gt_uninfected,\n",
    "            'GT_Total': gt_infected + gt_uninfected,\n",
    "            # Predictions\n",
    "            'Pred_Infected': pred_infected,\n",
    "            'Pred_Uninfected': pred_uninfected,\n",
    "            'Pred_Total': pred_infected + pred_uninfected,\n",
    "            # Performance Metrics (IoU-based)\n",
    "            'TP_Infected': tp_infected,\n",
    "            'FP_Infected': fp_infected,\n",
    "            'FN_Infected': fn_infected,\n",
    "            'TP_Uninfected': tp_uninfected,\n",
    "            'FP_Uninfected': fp_uninfected,\n",
    "            'FN_Uninfected': fn_uninfected,\n",
    "            # Confidence Statistics\n",
    "            'Infected_Conf_Mean': np.mean(pred_infected_conf) if pred_infected_conf else 0,\n",
    "            'Infected_Conf_Std': np.std(pred_infected_conf) if pred_infected_conf else 0,\n",
    "            'Infected_Conf_Min': min(pred_infected_conf) if pred_infected_conf else 0,\n",
    "            'Infected_Conf_Max': max(pred_infected_conf) if pred_infected_conf else 0,\n",
    "            'Uninfected_Conf_Mean': np.mean(pred_uninfected_conf) if pred_uninfected_conf else 0,\n",
    "            'Uninfected_Conf_Std': np.std(pred_uninfected_conf) if pred_uninfected_conf else 0,\n",
    "            'Uninfected_Conf_Min': min(pred_uninfected_conf) if pred_uninfected_conf else 0,\n",
    "            'Uninfected_Conf_Max': max(pred_uninfected_conf) if pred_uninfected_conf else 0,\n",
    "            # Low Confidence Analysis\n",
    "            'Low_Conf_Count': low_conf_count,\n",
    "            'Low_Conf_Details': '|'.join(low_conf_data) if low_conf_data else '',\n",
    "            # Calculated Metrics\n",
    "            'Infected_Recall': infected_recall,\n",
    "            'Infected_Precision': infected_precision,\n",
    "            'Infected_F1': infected_f1,\n",
    "            'Uninfected_Recall': uninfected_recall,\n",
    "            'Uninfected_Precision': uninfected_precision,\n",
    "            'Uninfected_F1': uninfected_f1,\n",
    "            # Decision Summary\n",
    "            'Decision_Quality': 'Good' if infected_recall > 0.7 else 'Poor',\n",
    "            'Visualization_Path': f'decision_{img_id}.png'\n",
    "        }\n",
    "        csv_data.append(csv_row)\n",
    "        \n",
    "        # CREATE VISUALIZATION (matching Cell 17 conventions)\n",
    "        fig = plt.figure(figsize=(24, 8))\n",
    "        gs = fig.add_gridspec(2, 6, hspace=0.15, wspace=0.2, \n",
    "                              height_ratios=[3, 1])\n",
    "        \n",
    "        fig.suptitle(f'Decision Analysis - {model_name} - Image ID: {img_id}', \n",
    "                    fontsize=16, fontweight='bold')\n",
    "        \n",
    "        # 1. ORIGINAL WITH GROUND TRUTH (SOLID LINES - matching Cell 17)\n",
    "        ax1 = fig.add_subplot(gs[0, :2])\n",
    "        ax1.imshow(img_array)\n",
    "        \n",
    "        if label_path.exists():\n",
    "            with open(label_path) as f:\n",
    "                for line in f:\n",
    "                    parts = line.strip().split()\n",
    "                    if len(parts) >= 5:\n",
    "                        cls_id = int(parts[0])\n",
    "                        cx, cy, w, h = map(float, parts[1:5])\n",
    "                        \n",
    "                        x1 = int((cx - w/2) * width)\n",
    "                        y1 = int((cy - h/2) * height)\n",
    "                        box_w = int(w * width)\n",
    "                        box_h = int(h * height)\n",
    "                        \n",
    "                        # Use consistent colors: GREEN for uninfected, RED for infected\n",
    "                        if cls_id == 0:\n",
    "                            color = 'green'\n",
    "                        else:\n",
    "                            color = 'red'\n",
    "                        \n",
    "                        # SOLID LINES for Ground Truth (matching Cell 17)\n",
    "                        rect = Rectangle((x1, y1), box_w, box_h, \n",
    "                                       linewidth=2, edgecolor=color, \n",
    "                                       facecolor='none', linestyle='-')  # SOLID LINE\n",
    "                        ax1.add_patch(rect)\n",
    "        \n",
    "        ax1.set_title(f'Ground Truth (Solid Lines)\\n(Infected: {gt_infected}, Uninfected: {gt_uninfected})', \n",
    "                     fontsize=12, fontweight='bold')\n",
    "        ax1.axis('off')\n",
    "        \n",
    "        # 2. MODEL PREDICTIONS (conf >= 0.5)\n",
    "        ax2 = fig.add_subplot(gs[0, 2])\n",
    "        viz_pred = img_array.copy()\n",
    "        \n",
    "        # Draw predictions with visual distinction\n",
    "        if results_normal.boxes is not None:\n",
    "            for box in results_normal.boxes:\n",
    "                x1, y1, x2, y2 = box.xyxy[0].int().tolist()\n",
    "                cls = int(box.cls.item())\n",
    "                conf = box.conf.item()\n",
    "                \n",
    "                # Consistent colors\n",
    "                if cls == 0:\n",
    "                    color = (0, 255, 0)  # Green for uninfected\n",
    "                else:\n",
    "                    color = (255, 0, 0)  # Red for infected\n",
    "                \n",
    "                # Draw prediction box (thicker to distinguish from GT)\n",
    "                cv2.rectangle(viz_pred, (x1, y1), (x2, y2), color, 4)\n",
    "                \n",
    "                # Add 'P' marker to indicate prediction\n",
    "                cv2.putText(viz_pred, f'P:{conf:.2f}', (x1, y1-5),\n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 3)\n",
    "        \n",
    "        ax2.imshow(viz_pred)\n",
    "        ax2.set_title(f'Predictions (Conf≥{CONF_HIGH})\\n(I: {pred_infected}, U: {pred_uninfected})', \n",
    "                     fontsize=11)\n",
    "        ax2.axis('off')\n",
    "        \n",
    "        # 3. LOW CONFIDENCE (0.3-0.5)\n",
    "        ax3 = fig.add_subplot(gs[0, 3])\n",
    "        viz_low = img_array.copy()\n",
    "        \n",
    "        additional_detections = 0\n",
    "        if results_low.boxes is not None:\n",
    "            for box in results_low.boxes:\n",
    "                x1, y1, x2, y2 = box.xyxy[0].int().tolist()\n",
    "                cls = int(box.cls.item())\n",
    "                conf = box.conf.item()\n",
    "                \n",
    "                if CONF_LOW <= conf < CONF_HIGH:  # Explicit uncertain range\n",
    "                    # Dimmer colors for uncertain\n",
    "                    color = (128, 0, 0) if cls == 1 else (0, 128, 0)\n",
    "                    cv2.rectangle(viz_low, (x1, y1), (x2, y2), color, 1)\n",
    "                    cv2.putText(viz_low, f'{conf:.2f}', (x1, y1-5),\n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 0.4, color, 1)\n",
    "                    additional_detections += 1\n",
    "                elif conf >= CONF_HIGH:\n",
    "                    color = (255, 0, 0) if cls == 1 else (0, 255, 0)\n",
    "                    cv2.rectangle(viz_low, (x1, y1), (x2, y2), color, 3)\n",
    "        \n",
    "        ax3.imshow(viz_low)\n",
    "        ax3.set_title(f'Uncertain [{CONF_LOW}-{CONF_HIGH})\\n+{additional_detections} uncertain', \n",
    "                     fontsize=11)\n",
    "        ax3.axis('off')\n",
    "        \n",
    "        # 4. DECISION HEATMAP WITH UNCERTAINTY (keeping yellow contours)\n",
    "        ax4 = fig.add_subplot(gs[0, 4:])\n",
    "        \n",
    "        decision_map = np.zeros((height, width), dtype=float)\n",
    "        uncertainty_map = np.zeros((height, width), dtype=float)\n",
    "        \n",
    "        if results_low.boxes is not None:\n",
    "            for box in results_low.boxes:\n",
    "                x1, y1, x2, y2 = box.xyxy[0].int().tolist()\n",
    "                conf = box.conf.item()\n",
    "                cls = int(box.cls.item())\n",
    "                \n",
    "                # Create decision map with proper coloring\n",
    "                if cls == 1:  # Infected - positive values (will be RED)\n",
    "                    decision_map[y1:y2, x1:x2] = np.maximum(\n",
    "                        decision_map[y1:y2, x1:x2], conf)\n",
    "                else:  # Uninfected - negative values (will be GREEN)\n",
    "                    decision_map[y1:y2, x1:x2] = np.minimum(\n",
    "                        decision_map[y1:y2, x1:x2], -conf)\n",
    "                \n",
    "                # Uncertainty for low confidence\n",
    "                if CONF_LOW < conf < 0.6:\n",
    "                    uncertainty_map[y1:y2, x1:x2] = 1 - abs(conf - 0.5) * 2\n",
    "        \n",
    "        # Smooth maps\n",
    "        decision_smooth = gaussian_filter(decision_map, sigma=5)\n",
    "        \n",
    "        # Create custom colormap: Green (negative) to White (0) to Red (positive)\n",
    "        from matplotlib.colors import LinearSegmentedColormap\n",
    "        colors_map = [(0, 0.5, 0), (1, 1, 1), (1, 0, 0)]  # Green -> White -> Red\n",
    "        n_bins = 100\n",
    "        cmap = LinearSegmentedColormap.from_list('GWR', colors_map, N=n_bins)\n",
    "        \n",
    "        ax4.imshow(img_array)\n",
    "        im = ax4.imshow(decision_smooth, cmap=cmap, alpha=0.6, vmin=-1, vmax=1)\n",
    "        \n",
    "        # Add uncertainty overlay (YELLOW CONTOURS)\n",
    "        uncertainty_smooth = gaussian_filter(uncertainty_map, sigma=3)\n",
    "        ax4.contour(uncertainty_smooth, levels=[0.5], colors='yellow', linewidths=2)\n",
    "        \n",
    "        ax4.set_title('Decision Confidence Map\\n(Red: Infected, Green: Uninfected, Yellow: Uncertain)', \n",
    "                     fontsize=11)\n",
    "        ax4.axis('off')\n",
    "        \n",
    "        # 5. CELL EXAMPLES (Bottom row)\n",
    "        examples = {'infected': [], 'uninfected': [], 'uncertain': []}\n",
    "        \n",
    "        if results_low.boxes is not None:\n",
    "            for box in results_low.boxes:\n",
    "                x1, y1, x2, y2 = box.xyxy[0].int().tolist()\n",
    "                cls = int(box.cls.item())\n",
    "                conf = box.conf.item()\n",
    "                \n",
    "                crop = img_array[max(0,y1):min(height,y2), max(0,x1):min(width,x2)]\n",
    "                if crop.size > 0:\n",
    "                    crop_resized = cv2.resize(crop, (64, 64))\n",
    "                    \n",
    "                    if CONF_LOW <= conf < CONF_HIGH:  # Explicit uncertain range\n",
    "                        examples['uncertain'].append((crop_resized, conf))\n",
    "                    elif cls == 1:\n",
    "                        examples['infected'].append((crop_resized, conf))\n",
    "                    else:\n",
    "                        examples['uninfected'].append((crop_resized, conf))\n",
    "        \n",
    "        # Display examples\n",
    "        for idx, (category, cells) in enumerate(examples.items()):\n",
    "            ax = fig.add_subplot(gs[1, idx*2:idx*2+2])\n",
    "            \n",
    "            if cells:\n",
    "                cells_sorted = sorted(cells, key=lambda x: x[1], reverse=True)[:3]\n",
    "                \n",
    "                if len(cells_sorted) >= 3:\n",
    "                    montage = np.hstack([c[0] for c in cells_sorted])\n",
    "                elif len(cells_sorted) == 2:\n",
    "                    montage = np.hstack([c[0] for c in cells_sorted])\n",
    "                else:\n",
    "                    montage = cells_sorted[0][0]\n",
    "                \n",
    "                ax.imshow(montage)\n",
    "                ax.set_title(f'{category.capitalize()} Examples\\n' + \n",
    "                           f'Conf: {\", \".join([f\"{c[1]:.2f}\" for c in cells_sorted])}',\n",
    "                           fontsize=10)\n",
    "            else:\n",
    "                ax.text(0.5, 0.5, f'No {category}\\ndetections', \n",
    "                       ha='center', va='center', fontsize=11)\n",
    "                ax.set_facecolor('lightgray')\n",
    "            \n",
    "            ax.axis('off')\n",
    "        \n",
    "        # Add colorbar\n",
    "        cbar_ax = fig.add_axes([0.92, 0.4, 0.02, 0.3])\n",
    "        cbar = plt.colorbar(im, cax=cbar_ax)\n",
    "        cbar.set_label('Decision Confidence', fontsize=10)\n",
    "        cbar.set_ticks([-1, -0.5, 0, 0.5, 1])\n",
    "        cbar.set_ticklabels(['Uninf\\n(High)', 'Uninf\\n(Low)', 'Neutral', \n",
    "                            'Inf\\n(Low)', 'Inf\\n(High)'])\n",
    "        \n",
    "        # Add legend matching Cell 17 style - MOVED TO THE RIGHT\n",
    "        from matplotlib.patches import Patch\n",
    "        legend_elements = [\n",
    "            Patch(facecolor='none', edgecolor='green', linewidth=2, \n",
    "                 linestyle='-', label='GT Uninfected'),\n",
    "            Patch(facecolor='none', edgecolor='red', linewidth=2, \n",
    "                 linestyle='-', label='GT Infected'),\n",
    "            Patch(facecolor='none', edgecolor='gray', linewidth=3, \n",
    "                 label='Predictions (P:)'),\n",
    "        ]\n",
    "        # LEGEND POSITIONED TO THE RIGHT, NEAR THE COLORBAR\n",
    "        ax4.legend(handles=legend_elements, \n",
    "                  loc='upper left',  # Anchor point on legend\n",
    "                  bbox_to_anchor=(1.02, 1.0),  # Position outside plot to the right\n",
    "                  fontsize=9)\n",
    "        \n",
    "        # SAVE ALL VISUALIZATIONS IN SAME FOLDER\n",
    "        fig_path = decision_dir / f'decision_{img_id}.png'\n",
    "        \n",
    "        if img_idx < num_samples_to_display:\n",
    "            # Display first 6 in notebook\n",
    "            fig.savefig(fig_path, dpi=300, bbox_inches='tight', facecolor='white')\n",
    "            plt.show()\n",
    "        else:\n",
    "            # Just save the rest without displaying\n",
    "            fig.savefig(fig_path, dpi=300, bbox_inches='tight', facecolor='white')\n",
    "            plt.close(fig)\n",
    "        \n",
    "        # Progress indicator\n",
    "        if (img_idx + 1) % 10 == 0:\n",
    "            print(f\"  Processed {img_idx + 1}/{len(test_images)} images...\")\n",
    "    \n",
    "    # Save THE ONE CSV with ALL data\n",
    "    csv_df = pd.DataFrame(csv_data)\n",
    "    csv_path = decision_dir / 'decision_analysis_complete.csv'\n",
    "    csv_df.to_csv(csv_path, index=False)\n",
    "    \n",
    "    # Also save as Excel\n",
    "    excel_path = decision_dir / 'decision_analysis_complete.xlsx'\n",
    "    csv_df.to_excel(excel_path, index=False)\n",
    "    \n",
    "    # Create simple summary for quick reference\n",
    "    summary = {\n",
    "        'Model': model_name,\n",
    "        'Total_Images': len(test_images),\n",
    "        'Mean_Infected_Recall': float(csv_df['Infected_Recall'].mean()),\n",
    "        'Mean_Infected_Precision': float(csv_df['Infected_Precision'].mean()),\n",
    "        'Mean_Infected_F1': float(csv_df['Infected_F1'].mean()),\n",
    "        'Images_With_Perfect_Detection': int((csv_df['Infected_Recall'] == 1.0).sum()),\n",
    "        'Images_With_Missed_Infections': int((csv_df['FN_Infected'] > 0).sum()),\n",
    "        'Total_Infected_Missed': int(csv_df['FN_Infected'].sum()),\n",
    "        'Total_False_Positives': int(csv_df['FP_Infected'].sum())\n",
    "    }\n",
    "    \n",
    "    # Save summary as JSON\n",
    "    json_path = decision_dir / 'summary.json'\n",
    "    with open(json_path, 'w') as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"DECISION ANALYSIS COMPLETE\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"✓ Images analyzed: {len(test_images)}\")\n",
    "    print(f\"✓ Images displayed: {num_samples_to_display}\")\n",
    "    print(f\"✓ All visualizations saved in: {decision_dir}\")\n",
    "    print(f\"✓ Single source CSV: {csv_path}\")\n",
    "    print(f\"✓ Excel version: {excel_path}\")\n",
    "    print(f\"✓ Summary: {json_path}\")\n",
    "    print(f\"✓ Visualization convention: GT=Solid, Pred=Marked(P:)\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    return csv_df, summary\n",
    "\n",
    "# Find test samples\n",
    "print(\"Finding test samples for decision analysis...\")\n",
    "test_samples = []\n",
    "img_dir = yolo_path / 'test' / 'images'\n",
    "lbl_dir = yolo_path / 'test' / 'labels'\n",
    "\n",
    "# Get ALL test images with annotations\n",
    "for img_path in img_dir.glob('*.jpg'):\n",
    "    label_path = lbl_dir / f\"{img_path.stem}.txt\"\n",
    "    if label_path.exists():\n",
    "        with open(label_path) as f:\n",
    "            if f.read().strip():  # Has annotations\n",
    "                test_samples.append(img_path)\n",
    "\n",
    "print(f\"Found {len(test_samples)} test images with annotations\")\n",
    "\n",
    "# Run analysis on ALL images, visualize only 6\n",
    "if test_samples:\n",
    "    results_dir = Path('../results')\n",
    "    \n",
    "    decision_df, decision_summary = analyze_model_decisions_enhanced(\n",
    "        evaluator.model,\n",
    "        test_samples,  # Pass ALL test samples\n",
    "        experiment_name,\n",
    "        results_dir,\n",
    "        num_samples_to_display=6  # Display only 6 in notebook\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nQuick Summary:\")\n",
    "    print(f\"  Mean Infected Recall: {decision_summary['Mean_Infected_Recall']:.3f}\")\n",
    "    print(f\"  Mean Infected F1: {decision_summary['Mean_Infected_F1']:.3f}\")\n",
    "    print(f\"  Images with missed infections: {decision_summary['Images_With_Missed_Infections']}/{decision_summary['Total_Images']}\")\n",
    "else:\n",
    "    print(\"No test samples found - check paths\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b883d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 19: Comprehensive W&B Logging - ORGANIZED STRUCTURE (COMPLETE FIXED VERSION)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPREHENSIVE W&B LOGGING - ORGANIZED\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if config.use_wandb:\n",
    "    import wandb\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from pathlib import Path\n",
    "    import json\n",
    "    \n",
    "    print(\"Organizing and logging all metrics to W&B...\")\n",
    "    \n",
    "    # =====================================\n",
    "    # SECTION 1: CHARTS (Metrics & Plots)\n",
    "    # =====================================\n",
    "    print(\"\\n[1/3] LOGGING CHARTS...\")\n",
    "    \n",
    "    # 1.1 Training Metrics (if available)\n",
    "    if 'training_metrics' in globals():\n",
    "        wandb.log({\n",
    "            'charts/training/total_time_minutes': training_metrics.get('total_training_time', 0)/60,\n",
    "            'charts/training/best_epoch': training_metrics.get('best_metrics', {}).get('epoch', 0),\n",
    "            'charts/training/final_train_loss': training_metrics.get('final_metrics', {}).get('train_loss', 0),\n",
    "            'charts/training/final_val_loss': training_metrics.get('final_metrics', {}).get('val_loss', 0),\n",
    "        })\n",
    "    \n",
    "    # 1.2 Global Performance - Validation\n",
    "    wandb.log({\n",
    "        'charts/validation/mAP50': val_results.get('global', {}).get('mAP50', 0),\n",
    "        'charts/validation/mAP50_95': val_results.get('global', {}).get('mAP50-95', 0),\n",
    "        'charts/validation/precision': val_results.get('global', {}).get('precision', 0),\n",
    "        'charts/validation/recall': val_results.get('global', {}).get('recall', 0),\n",
    "        'charts/validation/f1_score': val_results.get('global', {}).get('f1_score', 0),\n",
    "    })\n",
    "    \n",
    "    # 1.3 Global Performance - Test\n",
    "    wandb.log({\n",
    "        'charts/test/mAP50': test_results.get('global', {}).get('mAP50', 0),\n",
    "        'charts/test/mAP50_95': test_results.get('global', {}).get('mAP50-95', 0),\n",
    "        'charts/test/precision': test_results.get('global', {}).get('precision', 0),\n",
    "        'charts/test/recall': test_results.get('global', {}).get('recall', 0),\n",
    "        'charts/test/f1_score': test_results.get('global', {}).get('f1_score', 0),\n",
    "    })\n",
    "    \n",
    "    # 1.4 Per-Class Performance Charts - Validation\n",
    "    if 'per_class' in val_results:\n",
    "        for class_name in config.get_class_names().values():\n",
    "            if class_name in val_results['per_class']:\n",
    "                metrics = val_results['per_class'][class_name]\n",
    "                prefix = f\"charts/validation/{class_name.lower()}\"\n",
    "                wandb.log({\n",
    "                    f'{prefix}/mAP50': metrics.get('mAP50', 0),\n",
    "                    f'{prefix}/precision': metrics.get('precision', 0),\n",
    "                    f'{prefix}/recall': metrics.get('recall', 0),\n",
    "                    f'{prefix}/f1_score': metrics.get('f1_score', 0),\n",
    "                })\n",
    "    \n",
    "    # 1.5 Per-Class Performance Charts - Test\n",
    "    if 'per_class' in test_results:\n",
    "        for class_name in config.get_class_names().values():\n",
    "            if class_name in test_results['per_class']:\n",
    "                metrics = test_results['per_class'][class_name]\n",
    "                prefix = f\"charts/test/{class_name.lower()}\"\n",
    "                wandb.log({\n",
    "                    f'{prefix}/mAP50': metrics.get('mAP50', 0),\n",
    "                    f'{prefix}/precision': metrics.get('precision', 0),\n",
    "                    f'{prefix}/recall': metrics.get('recall', 0),\n",
    "                    f'{prefix}/f1_score': metrics.get('f1_score', 0),\n",
    "                })\n",
    "    \n",
    "    # 1.6 Inference Performance\n",
    "    if 'inference_results' in globals():\n",
    "        for batch_key, metrics in inference_results.items():\n",
    "            wandb.log({\n",
    "                f'charts/inference/{batch_key}/fps': metrics.get('fps', 0),\n",
    "                f'charts/inference/{batch_key}/mean_ms': metrics.get('mean_ms', 0),\n",
    "            })\n",
    "    \n",
    "    # =====================================\n",
    "    # SECTION 2: TABLES (Structured Data)\n",
    "    # =====================================\n",
    "    print(\"\\n[2/3] CREATING AND LOGGING TABLES...\")\n",
    "    \n",
    "    # 2.1 Per-Class Performance Table - Validation\n",
    "    val_class_data = []\n",
    "    for class_name in config.get_class_names().values():\n",
    "        if class_name in val_results.get('per_class', {}):\n",
    "            m = val_results['per_class'][class_name]\n",
    "            val_class_data.append({\n",
    "                'Class': class_name,\n",
    "                # 'mAP50': float(m.get('mAP50', 0)),  # FORCE FLOAT\n",
    "                'Precision': float(m.get('precision', 0)),\n",
    "                'Recall': float(m.get('recall', 0)),\n",
    "                'F1': float(m.get('f1_score', 0)),\n",
    "                'Support': int(m.get('support', 0))\n",
    "            })\n",
    "    if val_class_data:\n",
    "        wandb.log({\"tables/validation_per_class\": wandb.Table(dataframe=pd.DataFrame(val_class_data))})\n",
    "    \n",
    "    # 2.2 Per-Class Performance Table - Test\n",
    "    test_class_data = []\n",
    "    for class_name in config.get_class_names().values():\n",
    "        if class_name in test_results.get('per_class', {}):\n",
    "            m = test_results['per_class'][class_name]\n",
    "            test_class_data.append({\n",
    "                'Class': class_name,\n",
    "                # 'mAP50': float(m.get('mAP50', 0)),  # FORCE FLOAT\n",
    "                'Precision': float(m.get('precision', 0)),\n",
    "                'Recall': float(m.get('recall', 0)),\n",
    "                'F1': float(m.get('f1_score', 0)),\n",
    "                'AP': float(test_results.get('pr_analysis', {}).get(class_name, {}).get('ap', 0)),\n",
    "                'Support': int(m.get('support', 0)),\n",
    "                'TP': int(m.get('tp', 0)),\n",
    "                'FP': int(m.get('fp', 0)),\n",
    "                'FN': int(m.get('fn', 0))\n",
    "            })\n",
    "    if test_class_data:\n",
    "        wandb.log({\"tables/test_per_class\": wandb.Table(dataframe=pd.DataFrame(test_class_data))})\n",
    "    \n",
    "   # 2.3 TIDE Error Analysis Table - Aggregate\n",
    "if 'errors' in test_results:\n",
    "    error_data = []\n",
    "    for error_type, stats in test_results['errors'].items():\n",
    "        if isinstance(stats, dict):\n",
    "            error_data.append({\n",
    "                'Error_Type': error_type,\n",
    "                'Count': stats.get('count', 0),\n",
    "                'Rate_Per_Image': stats.get('rate_per_image', 0),\n",
    "                'Mean': stats.get('mean', 0),\n",
    "                'Std': stats.get('std', 0)\n",
    "            })\n",
    "    if error_data:\n",
    "        wandb.log({\"tables/error_analysis_aggregate\": wandb.Table(dataframe=pd.DataFrame(error_data))})\n",
    "\n",
    "# 2.3b TIDE Error Analysis Table - Per Class\n",
    "if 'errors_per_class' in test_results:\n",
    "    error_per_class_data = []\n",
    "    for class_name in config.get_class_names().values():\n",
    "        if class_name in test_results['errors_per_class']:\n",
    "            class_errors = test_results['errors_per_class'][class_name]\n",
    "            error_per_class_data.append({\n",
    "                'Class': class_name,\n",
    "                # Classification errors\n",
    "                'Classification_Count': class_errors.get('classification', {}).get('count', 0),\n",
    "                'Classification_Rate': class_errors.get('classification', {}).get('rate_per_image', 0),\n",
    "                # Localization errors  \n",
    "                'Localization_Count': class_errors.get('localization', {}).get('count', 0),\n",
    "                'Localization_Mean': class_errors.get('localization', {}).get('mean', 0),\n",
    "                # Background (False Positive) errors\n",
    "                'Background_FP_Count': class_errors.get('background', {}).get('count', 0),\n",
    "                'Background_FP_Rate': class_errors.get('background', {}).get('rate_per_image', 0),\n",
    "                # Missed (False Negative) errors\n",
    "                'Missed_FN_Count': class_errors.get('missed', {}).get('count', 0),\n",
    "                'Missed_FN_Rate': class_errors.get('missed', {}).get('rate_per_image', 0),\n",
    "                # Duplicate errors\n",
    "                'Duplicate_Count': class_errors.get('duplicate', {}).get('count', 0),\n",
    "                'Duplicate_Rate': class_errors.get('duplicate', {}).get('rate_per_image', 0),\n",
    "            })\n",
    "    if error_per_class_data:\n",
    "        wandb.log({\"tables/error_analysis_per_class\": wandb.Table(dataframe=pd.DataFrame(error_per_class_data))})\n",
    "        error_data = []\n",
    "        for error_type, stats in test_results['errors'].items():\n",
    "            if isinstance(stats, dict):\n",
    "                error_data.append({\n",
    "                    'Error_Type': error_type,\n",
    "                    'Count': stats.get('count', 0),\n",
    "                    'Rate_Per_Image': stats.get('rate_per_image', 0),\n",
    "                    'Mean': stats.get('mean', 0),\n",
    "                    'Std': stats.get('std', 0)\n",
    "                })\n",
    "        if error_data:\n",
    "            wandb.log({\"tables/error_analysis\": wandb.Table(dataframe=pd.DataFrame(error_data))})\n",
    "    \n",
    "    # 2.4 Prevalence-Stratified Analysis Table\n",
    "    if config.task == 'binary' and 'stratified' in test_results:\n",
    "        strat_data = []\n",
    "        for bin_name in ['0-1%', '1-3%', '3-5%', '>5%']:\n",
    "            if bin_name in test_results['stratified']:\n",
    "                stats = test_results['stratified'][bin_name]\n",
    "                strat_data.append({\n",
    "                    'Parasitemia_Bin': bin_name,\n",
    "                    'Mean_Recall': stats.get('mean_recall', 0),\n",
    "                    'Std_Recall': stats.get('std_recall', 0),\n",
    "                    'Count': stats.get('count', 0),\n",
    "                    'Clinical_Significance': \n",
    "                        'Critical' if '0-1' in bin_name or '1-3' in bin_name \n",
    "                        else 'Important' if '3-5' in bin_name \n",
    "                        else 'Standard'\n",
    "                })\n",
    "        if strat_data:\n",
    "            wandb.log({\"tables/stratified_analysis\": wandb.Table(dataframe=pd.DataFrame(strat_data))})\n",
    "    \n",
    "    # 2.5 Recall Variability Table (Per-Image Analysis)\n",
    "    if 'recall_variability' in test_results and 'wandb_table' in test_results['recall_variability']:\n",
    "        recall_df = pd.DataFrame(test_results['recall_variability']['wandb_table'])\n",
    "        wandb.log({\"tables/recall_variability_per_image\": wandb.Table(dataframe=recall_df)})\n",
    "    \n",
    "    # 2.6 Confusion Matrix - Test Only\n",
    "    if 'confusion' in test_results:\n",
    "        cm = np.array(test_results['confusion'])\n",
    "        class_names = list(config.get_class_names().values()) + ['Background/Missed']\n",
    "        cm_df = pd.DataFrame(cm, index=class_names, columns=class_names)\n",
    "        wandb.log({\"tables/test_confusion_matrix\": wandb.Table(dataframe=cm_df)})\n",
    "    \n",
    "    # 2.7 Validation-Test Comparison Table\n",
    "    comparison_data = []\n",
    "    comparison_data.append({\n",
    "        'Split': 'Validation',\n",
    "        'mAP50': float(val_results.get('global', {}).get('mAP50', 0)),\n",
    "        'mAP50_95': float(val_results.get('global', {}).get('mAP50-95', 0)),\n",
    "        'Uninfected_F1': float(val_results.get('per_class', {}).get('Uninfected', {}).get('f1_score', 0)),\n",
    "        'Infected_F1': float(val_results.get('per_class', {}).get('Infected', {}).get('f1_score', 0)),\n",
    "    })\n",
    "    comparison_data.append({\n",
    "        'Split': 'Test',\n",
    "        'mAP50': float(test_results.get('global', {}).get('mAP50', 0)),\n",
    "        'mAP50_95': float(test_results.get('global', {}).get('mAP50-95', 0)),\n",
    "        'Uninfected_F1': float(test_results.get('per_class', {}).get('Uninfected', {}).get('f1_score', 0)),\n",
    "        'Infected_F1': float(test_results.get('per_class', {}).get('Infected', {}).get('f1_score', 0)),\n",
    "    })\n",
    "    # Add difference row\n",
    "    comparison_data.append({\n",
    "        'Split': 'Difference (Val-Test)',\n",
    "        'mAP50': comparison_data[0]['mAP50'] - comparison_data[1]['mAP50'],\n",
    "        'mAP50_95': comparison_data[0]['mAP50_95'] - comparison_data[1]['mAP50_95'],\n",
    "        'Uninfected_F1': comparison_data[0]['Uninfected_F1'] - comparison_data[1]['Uninfected_F1'],\n",
    "        'Infected_F1': comparison_data[0]['Infected_F1'] - comparison_data[1]['Infected_F1'],\n",
    "    })\n",
    "    wandb.log({\"tables/val_test_comparison\": wandb.Table(dataframe=pd.DataFrame(comparison_data))})\n",
    "    \n",
    "    # 2.8 Decision Analysis Table\n",
    "    decision_csv = Path('../results') / experiment_name / 'decision_analysis' / 'decision_analysis_complete.csv'\n",
    "    if decision_csv.exists():\n",
    "        try:\n",
    "            decision_df = pd.read_csv(decision_csv)\n",
    "            wandb.log({\"tables/decision_analysis\": wandb.Table(dataframe=decision_df)})\n",
    "            print(\"  ✓ Logged decision analysis table\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Warning: Could not log decision analysis table: {e}\")\n",
    "    else:\n",
    "        print(f\"  Decision analysis CSV not found at {decision_csv}\")\n",
    "    \n",
    "    # 2.9 Precision-Recall Analysis Table\n",
    "    if 'pr_analysis' in test_results:\n",
    "        pr_data = []\n",
    "        for class_name, pr_stats in test_results['pr_analysis'].items():\n",
    "            pr_data.append({\n",
    "                'Class': class_name,\n",
    "                'AP': pr_stats.get('ap', 0),\n",
    "                'Optimal_Threshold': pr_stats.get('optimal_threshold', 0),\n",
    "                'Precision_at_Optimal': pr_stats.get('precision_at_optimal', 0),\n",
    "                'Recall_at_Optimal': pr_stats.get('recall_at_optimal', 0),\n",
    "                'Max_F1': pr_stats.get('max_f1', 0)\n",
    "            })\n",
    "        if pr_data:\n",
    "            wandb.log({\"tables/precision_recall_analysis\": wandb.Table(dataframe=pd.DataFrame(pr_data))})\n",
    "    \n",
    "    # =====================================\n",
    "    # SECTION 3: ARTIFACTS & IMAGES\n",
    "    # =====================================\n",
    "    print(\"\\n[3/3] LOGGING ARTIFACTS AND VISUALIZATIONS...\")\n",
    "    \n",
    "    # 3.1 Create main artifact for all visualizations\n",
    "    viz_artifact = wandb.Artifact(\n",
    "        name=f\"visualizations_{experiment_name}\",\n",
    "        type=\"visualizations\",\n",
    "        metadata={\n",
    "            'model': config.model_name,\n",
    "            'task': config.task,\n",
    "            'strategy': experiment_name.split('_')[-1]\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # 3.2 Add all PNG visualizations\n",
    "    viz_path = Path('../results') / experiment_name\n",
    "    viz_count = 0\n",
    "    \n",
    "    # Main directory images\n",
    "    for img_file in viz_path.glob('*.png'):\n",
    "        viz_artifact.add_file(str(img_file))\n",
    "        # Also log as image for quick viewing\n",
    "        wandb.log({f\"images/{img_file.stem}\": wandb.Image(str(img_file))})\n",
    "        viz_count += 1\n",
    "    \n",
    "    # Evaluation subdirectory\n",
    "    eval_path = viz_path / 'evaluation'\n",
    "    if eval_path.exists():\n",
    "        for img_file in eval_path.glob('*.png'):\n",
    "            viz_artifact.add_file(str(img_file))\n",
    "            wandb.log({f\"images/evaluation/{img_file.stem}\": wandb.Image(str(img_file))})\n",
    "            viz_count += 1\n",
    "    \n",
    "    # Decision analysis images (only first 4 for quick view)\n",
    "    decision_path = viz_path / 'decision_analysis'\n",
    "    if decision_path.exists():\n",
    "        for idx, img_file in enumerate(sorted(decision_path.glob('decision_*.png'))):\n",
    "            viz_artifact.add_file(str(img_file))\n",
    "            if idx < 4:  # Only log first 4 to W&B for quick viewing\n",
    "                wandb.log({f\"images/decision/{img_file.stem}\": wandb.Image(str(img_file))})\n",
    "            viz_count += 1\n",
    "    \n",
    "    wandb.log_artifact(viz_artifact)\n",
    "    print(f\"  Logged visualization artifact with {viz_count} images\")\n",
    "    \n",
    "    # 3.3 Model artifact with comprehensive metadata\n",
    "    if 'best_model_path' in globals() and Path(best_model_path).exists():\n",
    "        model_artifact = wandb.Artifact(\n",
    "            name=f\"{config.dataset}_{config.task}_{config.model_name}_model\",\n",
    "            type=\"model\",\n",
    "            metadata={\n",
    "                # Performance metrics\n",
    "                'test_mAP50': test_results.get('global', {}).get('mAP50', 0),\n",
    "                'test_mAP50_95': test_results.get('global', {}).get('mAP50-95', 0),\n",
    "                'infected_recall': test_results.get('per_class', {}).get('Infected', {}).get('recall', 0),\n",
    "                'infected_f1': test_results.get('per_class', {}).get('Infected', {}).get('f1_score', 0),\n",
    "                \n",
    "                # Clinical metrics\n",
    "                'low_parasitemia_recall': test_results.get('stratified', {}).get('1-3%', {}).get('mean_recall', 0),\n",
    "                \n",
    "                # Training info\n",
    "                'epochs_trained': config.epochs,\n",
    "                'training_time_minutes': training_metrics.get('total_training_time', 0)/60 if 'training_metrics' in globals() else 0,\n",
    "                'strategy': experiment_name.split('_')[-1],\n",
    "            }\n",
    "        )\n",
    "        model_artifact.add_file(str(best_model_path))\n",
    "        \n",
    "        # Add config\n",
    "        config_path = Path('../results') / experiment_name / 'config.json'\n",
    "        if config_path.exists():\n",
    "            model_artifact.add_file(str(config_path))\n",
    "        \n",
    "        wandb.log_artifact(model_artifact)\n",
    "        print(\"  Logged model artifact\")\n",
    "    \n",
    "    # 3.4 Data artifacts (CSVs for cross-model comparison)\n",
    "    data_artifact = wandb.Artifact(\n",
    "        name=f\"analysis_data_{experiment_name}\",\n",
    "        type=\"analysis_data\",\n",
    "        metadata={'model': experiment_name}\n",
    "    )\n",
    "    \n",
    "    # Add recall variability CSV\n",
    "    recall_csv = Path('../results') / experiment_name / 'recall_variability_data.csv'\n",
    "    if recall_csv.exists():\n",
    "        data_artifact.add_file(str(recall_csv))\n",
    "    \n",
    "    # Add decision analysis CSV\n",
    "    if decision_csv.exists():\n",
    "        data_artifact.add_file(str(decision_csv))\n",
    "    \n",
    "    wandb.log_artifact(data_artifact)\n",
    "    print(\"  Logged data artifacts\")\n",
    "    \n",
    "    # =====================================\n",
    "    # FINAL SUMMARY\n",
    "    # =====================================\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"W&B LOGGING COMPLETE\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"✓ Charts: Training, validation, test, and inference metrics\")\n",
    "    print(f\"✓ Tables: 9 comprehensive tables created\")\n",
    "    print(f\"✓ Artifacts: {viz_count} visualizations + model + data files\")\n",
    "    print(f\"✓ Run URL: {wandb.run.url}\")\n",
    "    print(\"\\nOrganization:\")\n",
    "    print(\"  - charts/: All performance metrics\")\n",
    "    print(\"  - tables/: All structured data\")\n",
    "    print(\"  - images/: Quick view visualizations\")\n",
    "    print(\"  - artifacts/: Complete files for download\")\n",
    "    \n",
    "    wandb.finish()\n",
    "    print(\"\\n✓ W&B run completed successfully\")\n",
    "    \n",
    "else:\n",
    "    print(\"W&B logging disabled - skipping\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
